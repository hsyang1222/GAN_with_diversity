{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e271bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [on]\n",
      "Loading model from: /opt/conda/envs/stylegan2/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "{'dataset': 'cifar10', 'dataroot': '../../dataset', 'workers': 4, 'batchSize': 2048, 'imageSize': 32, 'nz': 100, 'ngf': 64, 'ndf': 64, 'niter': 100, 'lr': 0.0002, 'beta1': 0.5, 'cuda': True, 'dry_run': False, 'ngpu': 1, 'netG': '', 'netD': '', 'netE': '', 'manualSeed': None, 'classes': None, 'outf': 'result_image', 'AEiter': 0, 'z_add': 0.8, 'lambda_diverse': 0.0, 'lambda_uniform': 0, 'try_div_chance': 0, 'device': 'cuda:1', 'name': 'keep_chance', 'report_every': 10, 'keep_try_over': 0.8}\n",
      "Random Seed:  4832\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Encoder(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(512, 100, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "    (12): Tanh()\n",
      "  )\n",
      ")\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../../dataset\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=32, interpolation=bilinear)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "Using exist inception model info from : ../../../inception_model_info/f416f90db7a0eece1329f3f133efc9aa.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhsyang1222\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">keep_chance</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hsyang1222/GAN_report\" target=\"_blank\">https://wandb.ai/hsyang1222/GAN_report</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hsyang1222/GAN_report/runs/fc9r03if\" target=\"_blank\">https://wandb.ai/hsyang1222/GAN_report/runs/fc9r03if</a><br/>\n",
       "                Run data is saved locally in <code>/home/hsyang/workspace/20210306_gan/GAN_with_diversity/dcgan_modi/wandb/run-20210521_181504-fc9r03if</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import generative_model_score\n",
    "inception_model_score = generative_model_score.GenerativeModelScore()\n",
    "inception_model_score.lazy_mode(True)\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import lpips\n",
    "\n",
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "    'dataset':'cifar10',\n",
    "    'dataroot':'../../dataset',\n",
    "    'workers':4,\n",
    "    'batchSize':2048,\n",
    "    'imageSize':32,\n",
    "    'nz':100,\n",
    "    'ngf':64,\n",
    "    'ndf':64,\n",
    "    'niter':100,\n",
    "    'lr':0.0002,\n",
    "    'beta1':0.5,\n",
    "    'cuda':True,\n",
    "    'dry_run':False,\n",
    "    'ngpu':1,\n",
    "    'netG':'',\n",
    "    'netD':'',\n",
    "    'netE':'',\n",
    "    'manualSeed':None,\n",
    "    'classes':None,\n",
    "    'outf':'result_image',\n",
    "    'AEiter' : 0,\n",
    "    'z_add':0.8,\n",
    "    'lambda_diverse': 0.0,\n",
    "    'lambda_uniform' : 0,\n",
    "    'try_div_chance' : 0,\n",
    "    'device':'cuda:1',\n",
    "    'name' : 'keep_chance',\n",
    "    'report_every' : 10,\n",
    "    'keep_try_over' : 0.8\n",
    "})\n",
    "\n",
    "lpips_model = loss_fn = lpips.LPIPS(net='alex', spatial=True)\n",
    "\n",
    "#opt = parser.parse_args()\n",
    "opt = args\n",
    "print(opt)\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "  \n",
    "\n",
    "if opt.dataroot is None and str(opt.dataset).lower() != 'fake':\n",
    "    raise ValueError(\"`dataroot` parameter is required for dataset \\\"%s\\\"\" % opt.dataset)\n",
    "\n",
    "if opt.dataset in ['imagenet', 'folder', 'lfw']:\n",
    "    # folder dataset\n",
    "    dataset = dset.ImageFolder(root=opt.dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(opt.imageSize),\n",
    "                                   transforms.CenterCrop(opt.imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "    nc=3\n",
    "elif opt.dataset == 'lsun':\n",
    "    classes = [ c + '_train' for c in opt.classes.split(',')]\n",
    "    dataset = dset.LSUN(root=opt.dataroot, classes=classes,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize(opt.imageSize),\n",
    "                            transforms.CenterCrop(opt.imageSize),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ]))\n",
    "    nc=3\n",
    "elif opt.dataset == 'cifar10':\n",
    "    dataset = dset.CIFAR10(root=opt.dataroot, #download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(opt.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    nc=3\n",
    "\n",
    "elif opt.dataset == 'mnist':\n",
    "        dataset = dset.MNIST(root=opt.dataroot, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(opt.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,)),\n",
    "                           ]))\n",
    "        nc=1\n",
    "\n",
    "elif opt.dataset == 'fake':\n",
    "    dataset = dset.FakeData(image_size=(3, opt.imageSize, opt.imageSize),\n",
    "                            transform=transforms.ToTensor())\n",
    "    nc=3\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "\n",
    "device = torch.device(opt.device if opt.cuda else \"cpu\")\n",
    "ngpu = int(opt.ngpu)\n",
    "nz = int(opt.nz)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            #nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 2, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 2 x 2\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 16 x 16\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "if opt.netG != '':\n",
    "    netG.load_state_dict(torch.load(opt.netG))\n",
    "print(netG)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            #nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Conv2d(ndf * 8, 1, 2, 1, 0, bias=False),\n",
    "            # state size. 1x1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            #nn.Conv2d(ndf * 8, 100, 4, 1, 0, bias=False),\n",
    "            nn.Conv2d(ndf * 8, nz, 2, 1, 0, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "        \n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "if opt.netD != '':\n",
    "    netD.load_state_dict(torch.load(opt.netD))\n",
    "print(netD)\n",
    "\n",
    "netE = Encoder(ngpu).to(device)\n",
    "netE.apply(weights_init)\n",
    "if opt.netE != '':\n",
    "    netE.load_state_dict(torch.load(opt.netE))\n",
    "print(netE)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerE = optim.Adam(netE.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "\n",
    "train_loader = dataloader\n",
    "print(train_loader.dataset)\n",
    "real_images_info_file_name = inception_model_score.trainloaderinfo_to_hashedname(train_loader)\n",
    "real_image_info_path = '../../../inception_model_info/'+real_images_info_file_name\n",
    "\n",
    "if os.path.exists( real_image_info_path) : \n",
    "    print(\"Using exist inception model info from :\",real_image_info_path)\n",
    "    inception_model_score.load_real_images_info(real_image_info_path)\n",
    "else : \n",
    "    inception_model_score.model_to(device)\n",
    "\n",
    "    #put real image\n",
    "    for each_batch in train_loader : \n",
    "        X_train_batch = each_batch[0]\n",
    "        inception_model_score.put_real(X_train_batch)\n",
    "\n",
    "    #generate real images info\n",
    "    inception_model_score.lazy_forward(batch_size=64, device=device, real_forward=True)\n",
    "    inception_model_score.calculate_real_image_statistics()\n",
    "    #save real images info for next experiments\n",
    "    inception_model_score.save_real_images_info(real_image_info_path)\n",
    "    print(\"Save inception model info to :\", real_images_info_file_name)\n",
    "    #offload inception_model\n",
    "    inception_model_score.model_to('cpu')\n",
    "    \n",
    "import wandb\n",
    "wandb.init(project='GAN_report', name=opt.name, config=opt)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ce928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mse = torch.nn.MSELoss()\n",
    "\n",
    "import tqdm\n",
    "for epoch in tqdm.tqdm(range(config.AEiter), desc=\"AE\"):\n",
    "    loss_sum = 0.\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_cuda = data[0].to(device)\n",
    "        batch_size = real_cuda.size(0)\n",
    "        \n",
    "        latent_vector = netE(real_cuda)\n",
    "        latent_4dim = latent_vector.view(batch_size,nz,1,1)\n",
    "        repaint = netG(latent_4dim)\n",
    "        \n",
    "        mse_loss = mse(repaint, real_cuda)\n",
    "        optimizerE.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        mse_loss.backward()\n",
    "        optimizerE.step()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        loss_sum += mse_loss.item()\n",
    "        \n",
    "del netE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf80c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep try: 0.16969999999999996 < 0.8\n",
      "keep try: 1.0 < 0.8 (pass)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/stylegan2/lib/python3.9/site-packages/torch/nn/functional.py:3502: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate fake images info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:11<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num real: 10000 Num fake: 10000\n",
      "[0/100][24/25] Loss_D: 0.3245 Loss_G: 0.0071 D(x): 0.8798 D(G(z)): 0.1641 / 0.9929, DivLoss : 0.0106\n",
      "\t\tFID : 429.8828, IS_f : 1.1670, P : 0.0013, R : 0.0000, D : 0.0003, C : 0.0001, LPIPS : 0.0002\n",
      "keep try: 0.0 < 0.8\n",
      "keep try: 1.0 < 0.8 (pass)\n",
      "keep try: 0.0 < 0.8\n",
      "keep try: 0.87942 < 0.8 (pass)\n",
      "keep try: 0.0 < 0.8\n",
      "keep try: 0.81796 < 0.8 (pass)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fd07badc35e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_diverse\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mD_G_z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0moptimizerG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.niter):\n",
    "    under50z_list = []\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label,\n",
    "                           dtype=real_cpu.dtype, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        fake2 = netG(noise + config.z_add)\n",
    "        \n",
    "        if config.report_every > 0 and epoch % config.report_every == 0 :\n",
    "            inception_model_score.put_fake(fake.detach().cpu())\n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        \n",
    "        fake = netG(noise)\n",
    "        fake2 = netG(noise + config.z_add)\n",
    "        \n",
    "        output = netD(fake)\n",
    "        \n",
    "        loss_ds = torch.mean(torch.abs(fake - fake2))\n",
    "        \n",
    "        errG = criterion(output, label) - config.lambda_diverse * loss_ds\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        fake = netG(noise)\n",
    "        output = netD(fake)\n",
    "        condition_under50 = output < 0.5\n",
    "        under50z_list.append( noise[condition_under50].detach().cpu() )\n",
    "        \n",
    "    try_retrain = 0\n",
    "    netD.eval()\n",
    "        \n",
    "\n",
    "    if len(under50z_list) == 0 :\n",
    "        num_of_under50 = 0\n",
    "        num_of_first_under50 = 0\n",
    "    else:\n",
    "        under50z_set = torch.cat(under50z_list)\n",
    "        num_of_under50 = under50z_set.size(0)\n",
    "        num_of_first_under50 = num_of_under50\n",
    "        \n",
    "\n",
    "    #another chance part\n",
    "    '''\n",
    "    while try_retrain < config.try_div_chance :\n",
    "        print(try_retrain, under50z_set.shape, num_of_under50/50000)\n",
    "        under50z_dataset = torch.utils.data.TensorDataset(under50z_set)\n",
    "        under50z_dataloder = torch.utils.data.DataLoader(under50z_dataset,batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "        under50z_list = []\n",
    "\n",
    "        for data in under50z_dataloder : \n",
    "            z = data[0].to(device)\n",
    "            netG.zero_grad()\n",
    "            batch_size = z.size(0)\n",
    "            label = torch.full((batch_size,), real_label,\n",
    "                           dtype=z.dtype, device=device)\n",
    "\n",
    "            fake = netG(z)\n",
    "            output = netD(fake)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            fake = netG(z)\n",
    "            output = netD(fake)\n",
    "            condition_under50 = output < 0.5\n",
    "            under50z_list.append( z[condition_under50].detach().cpu() )\n",
    "\n",
    "        try_retrain+=1\n",
    "\n",
    "        if len(under50z_list) == 0 :\n",
    "            num_of_under50 = 0\n",
    "            break\n",
    "        else:\n",
    "            under50z_set = torch.cat(under50z_list)\n",
    "            num_of_under50 = under50z_set.size(0)\n",
    "     '''\n",
    "\n",
    "    #keep try G part\n",
    "    keep_try = True\n",
    "    while (1-num_of_under50/50000) < config.keep_try_over : \n",
    "        print(\"keep try:\", try_retrain, (1-num_of_under50/50000), \"<\",config.keep_try_over)\n",
    "        num_of_under50 = 0\n",
    "        \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            batch_size = data[0].size(0)\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            label = torch.full((batch_size,), real_label,\n",
    "                           dtype=real_cpu.dtype, device=device)\n",
    "            fake = netG(noise)\n",
    "            fake2 = netG(noise + config.z_add)\n",
    "\n",
    "            output = netD(fake)\n",
    "\n",
    "            loss_ds = torch.mean(torch.abs(fake - fake2))\n",
    "\n",
    "            errG = criterion(output, label) - config.lambda_diverse * loss_ds\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "            fake = netG(noise)\n",
    "            output = netD(fake)\n",
    "            condition_under50 = output < 0.5\n",
    "            num_of_under50 += int(torch.sum(condition_under50).detach().cpu())\n",
    "\n",
    "        try_retrain += 1\n",
    "    else :\n",
    "        print(\"keep try:\", try_retrain, (1-num_of_under50/50000), \"<\",config.keep_try_over,\"(pass)\")\n",
    "        \n",
    "\n",
    "    netD.train()\n",
    "    num_of_still_under50 = torch.cat(under50z_list).size(0)\n",
    "\n",
    "    if config.report_every > 0 and epoch % config.report_every == 0:\n",
    "        fake = netG(fixed_noise)\n",
    "        vutils.save_image(fake.detach(),\n",
    "            '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "            normalize=True)\n",
    "\n",
    "        fake2 = netG(fixed_noise + config.z_add)\n",
    "        vutils.save_image(fake2.detach(),\n",
    "            '%s/fake2_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "            normalize=True)\n",
    "        \n",
    "        fake_np = vutils.make_grid(fake.detach().cpu(), nrow=32).permute(1,2,0).numpy()\n",
    "        fake2_np = vutils.make_grid(fake2.detach().cpu(), nrow=32).permute(1,2,0).numpy()\n",
    "        \n",
    "        netG = netG.to('cpu')\n",
    "        netD = netD.to('cpu')\n",
    "        \n",
    "        lpips_model.to(device)\n",
    "        ex_d = lpips_model.forward(fake, fake2).mean()\n",
    "        lpips_model.to('cpu')\n",
    "        \n",
    "        inception_model_score.model_to(device)\n",
    "\n",
    "        #generate fake images info\n",
    "        inception_model_score.lazy_forward(batch_size=64, device=device, fake_forward=True)\n",
    "        inception_model_score.calculate_fake_image_statistics()\n",
    "        metrics = inception_model_score.calculate_generative_score()\n",
    "        inception_model_score.clear_fake()\n",
    "\n",
    "        #onload all GAN model to cpu and offload inception model to gpu\n",
    "        inception_model_score.model_to('cpu')\n",
    "        netG = netG.to(device)\n",
    "        netD = netD.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f, DivLoss : %.4f'\n",
    "          % (epoch, opt.niter, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2, loss_ds.item()))\n",
    "        \n",
    "        print(\"\\t\\tFID : %.4f, IS_f : %.4f, P : %.4f, R : %.4f, D : %.4f, C : %.4f, LPIPS : %.4f, \n",
    "              %(metrics['fid'], metrics['fake_is'], metrics['precision'], \n",
    "                metrics['recall'], metrics['density'], metrics['coverage'], ex_d))\n",
    "        \n",
    "       \n",
    "        vutils.save_image(real_cpu,\n",
    "                '%s/real_samples.png' % opt.outf,\n",
    "                normalize=True)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\" : epoch,\n",
    "            \"Loss_D\": errD.item(),\n",
    "            \"Loss_G\": errG.item(),\n",
    "            \"D(real)\": D_x,\n",
    "            \"D(G(z))-before D train\": D_G_z1,\n",
    "            \"D(G(z))-after D train\": D_G_z2,\n",
    "            \"DivLoss\" : loss_ds.item(),\n",
    "            \"fid\" : metrics['fid'],\n",
    "            'fake_is':metrics['fake_is'],\n",
    "            \"precision\":metrics['precision'],\n",
    "            \"recall\":metrics['recall'],\n",
    "            \"density\":metrics['density'],\n",
    "            \"coverage\":metrics['coverage'],\n",
    "            \"G(z) \" : [wandb.Image(fake_np, caption='fixed z image')],\n",
    "            \"G(z + div_add) \" : [wandb.Image(fake2_np, caption='fixed z + 1e-6 image')],\n",
    "            'num_of_still_under50' : num_of_still_under50,\n",
    "            'num_of_first_under50' : num_of_first_under50,\n",
    "            'LPIPS' : ex_d,\n",
    "            'try_retrain' : try_retrain\n",
    "        })\n",
    "\n",
    "        if opt.dry_run:\n",
    "            break\n",
    "    # do checkpointing\n",
    "    '''\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        vutils.save_image(fake.detach(),\n",
    "                '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                normalize=True)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad70f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan2",
   "language": "python",
   "name": "stylegan2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
