{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'cifar10', 'dataroot': '../dataset', 'workers': 2, 'batchSize': 2048, 'imageSize': 64, 'nz': 100, 'ngf': 64, 'ndf': 64, 'niter': 100, 'lr': 0.0002, 'beta1': 0.5, 'cuda': True, 'dry_run': False, 'ngpu': 1, 'netG': '', 'netD': '', 'netE': '', 'manualSeed': None, 'classes': None, 'outf': 'result_image', 'AEiter': 1}\n",
      "Random Seed:  6935\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Encoder(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(512, 100, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Tanh()\n",
      "  )\n",
      ")\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../dataset\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=64, interpolation=bilinear)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "Using exist inception model info from : 0652f989f7f2a23771a3bf715c193582.pickle\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from dcgan import generative_model_score\n",
    "inception_model_score = generative_model_score.GenerativeModelScore()\n",
    "inception_model_score.lazy_mode(True)\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "    'dataset':'cifar10',\n",
    "    'dataroot':'../dataset',\n",
    "    'workers':2,\n",
    "    'batchSize':2048,\n",
    "    'imageSize':64,\n",
    "    'nz':100,\n",
    "    'ngf':64,\n",
    "    'ndf':64,\n",
    "    'niter':100,\n",
    "    'lr':0.0002,\n",
    "    'beta1':0.5,\n",
    "    'cuda':True,\n",
    "    'dry_run':False,\n",
    "    'ngpu':1,\n",
    "    'netG':'',\n",
    "    'netD':'',\n",
    "    'netE':'',\n",
    "    'manualSeed':None,\n",
    "    'classes':None,\n",
    "    'outf':'result_image',\n",
    "    'AEiter' : 1\n",
    "})\n",
    "\n",
    "\n",
    "#opt = parser.parse_args()\n",
    "opt = args\n",
    "print(opt)\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "  \n",
    "\n",
    "if opt.dataroot is None and str(opt.dataset).lower() != 'fake':\n",
    "    raise ValueError(\"`dataroot` parameter is required for dataset \\\"%s\\\"\" % opt.dataset)\n",
    "\n",
    "if opt.dataset in ['imagenet', 'folder', 'lfw']:\n",
    "    # folder dataset\n",
    "    dataset = dset.ImageFolder(root=opt.dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(opt.imageSize),\n",
    "                                   transforms.CenterCrop(opt.imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "    nc=3\n",
    "elif opt.dataset == 'lsun':\n",
    "    classes = [ c + '_train' for c in opt.classes.split(',')]\n",
    "    dataset = dset.LSUN(root=opt.dataroot, classes=classes,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize(opt.imageSize),\n",
    "                            transforms.CenterCrop(opt.imageSize),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ]))\n",
    "    nc=3\n",
    "elif opt.dataset == 'cifar10':\n",
    "    dataset = dset.CIFAR10(root=opt.dataroot, #download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(opt.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    nc=3\n",
    "\n",
    "elif opt.dataset == 'mnist':\n",
    "        dataset = dset.MNIST(root=opt.dataroot, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(opt.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,)),\n",
    "                           ]))\n",
    "        nc=1\n",
    "\n",
    "elif opt.dataset == 'fake':\n",
    "    dataset = dset.FakeData(image_size=(3, opt.imageSize, opt.imageSize),\n",
    "                            transform=transforms.ToTensor())\n",
    "    nc=3\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "\n",
    "device = torch.device(\"cuda:1\" if opt.cuda else \"cpu\")\n",
    "ngpu = int(opt.ngpu)\n",
    "nz = int(opt.nz)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "if opt.netG != '':\n",
    "    netG.load_state_dict(torch.load(opt.netG))\n",
    "print(netG)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 100, 4, 1, 0, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "        \n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "if opt.netD != '':\n",
    "    netD.load_state_dict(torch.load(opt.netD))\n",
    "print(netD)\n",
    "\n",
    "netE = Encoder(ngpu).to(device)\n",
    "netE.apply(weights_init)\n",
    "if opt.netE != '':\n",
    "    netE.load_state_dict(torch.load(opt.netE))\n",
    "print(netE)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerE = optim.Adam(netE.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "\n",
    "train_loader = dataloader\n",
    "print(train_loader.dataset)\n",
    "import hashlib\n",
    "real_images_info_file_name = inception_model_score.trainloaderinfo_to_hashedname(train_loader)\n",
    "if os.path.exists('../../inception_model_info/' + real_images_info_file_name) : \n",
    "    print(\"Using exist inception model info from :\", real_images_info_file_name)\n",
    "    inception_model_score.load_real_images_info('../../inception_model_info/' + real_images_info_file_name)\n",
    "else : \n",
    "    inception_model_score.model_to('cuda')\n",
    "\n",
    "    #put real image\n",
    "    for each_batch in train_loader : \n",
    "        X_train_batch = each_batch[0]\n",
    "        inception_model_score.put_real(X_train_batch)\n",
    "\n",
    "    #generate real images info\n",
    "    inception_model_score.lazy_forward(batch_size=64, device='cuda', real_forward=True)\n",
    "    inception_model_score.calculate_real_image_statistics()\n",
    "    #save real images info for next experiments\n",
    "    inception_model_score.save_real_images_info('../../inception_model_info/' + real_images_info_file_name)\n",
    "    print(\"Save inception model info to :\", real_images_info_file_name)\n",
    "    #offload inception_model\n",
    "    inception_model_score.model_to('cpu')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2tjng2rz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33355<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/hsyang/workspace/20210306_gan/GAN_with_diversity/wandb/run-20210516_092802-2tjng2rz/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/hsyang/workspace/20210306_gan/GAN_with_diversity/wandb/run-20210516_092802-2tjng2rz/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">fine-violet-6</strong>: <a href=\"https://wandb.ai/hsyang1222/GAN_with_diversity%2BAE/runs/2tjng2rz\" target=\"_blank\">https://wandb.ai/hsyang1222/GAN_with_diversity%2BAE/runs/2tjng2rz</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2tjng2rz). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">stellar-durian-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hsyang1222/GAN_with_diversity%2BAE\" target=\"_blank\">https://wandb.ai/hsyang1222/GAN_with_diversity%2BAE</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hsyang1222/GAN_with_diversity%2BAE/runs/1vv5wtzg\" target=\"_blank\">https://wandb.ai/hsyang1222/GAN_with_diversity%2BAE/runs/1vv5wtzg</a><br/>\n",
       "                Run data is saved locally in <code>/home/hsyang/workspace/20210306_gan/GAN_with_diversity/wandb/run-20210516_092829-1vv5wtzg</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='GAN_with_diversity+AE', config=opt)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))\n",
    "\n",
    "netE.zero_grad()\n",
    "netG.zero_grad()\n",
    "\n",
    "real_cpu = data[0].to(device)\n",
    "batch_size = real_cpu.size(0)\n",
    "\n",
    "latent_vector = netE(real_cpu)\n",
    "latent_4dim = latent_vector.view(batch_size,nz,1,1)\n",
    "repaint = netG(latent_4dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([204800]),\n",
       " torch.Size([2048, 100, 1, 1]),\n",
       " torch.Size([2048, 3, 64, 64]),\n",
       " torch.Size([2048, 3, 64, 64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vector.shape, latent_4dim.shape, repaint.shape, data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(config.AEiter):\n",
    "    repaint_list = []\n",
    "    loss_sum = 0.\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_cuda = data[0].to(device)\n",
    "        batch_size = real_cuda.size(0)\n",
    "        \n",
    "        latent_vector = netE(real_cuda)\n",
    "        latent_4dim = latent_vector.view(batch_size,nz,1,1)\n",
    "        repaint = netG(latent_4dim)\n",
    "        \n",
    "        mse_loss = mse(repaint, real_cuda)\n",
    "        optimizerE.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        mse_loss.backward()\n",
    "        optimizerE.step()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        loss_sum += mse_loss.item()\n",
    "        \n",
    "        repaint_list.append(repaint.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(opt.niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label,\n",
    "                           dtype=real_cpu.dtype, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        fake2 = netG(noise + 1e-6)\n",
    "        \n",
    "        if epoch % 10 == 0 :\n",
    "            inception_model_score.put_fake(fake.detach().cpu())\n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        \n",
    "        loss_ds = torch.mean(torch.abs(fake.detach() - fake2.detach()))\n",
    "        \n",
    "        errG = criterion(output, label) - 0.1 * loss_ds\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        netG = netG.to('cpu')\n",
    "        netD = netD.to('cpu')\n",
    "        inception_model_score.model_to(device)\n",
    "\n",
    "        #generate fake images info\n",
    "        inception_model_score.lazy_forward(batch_size=64, device=device, fake_forward=True)\n",
    "        inception_model_score.calculate_fake_image_statistics()\n",
    "        metrics = inception_model_score.calculate_generative_score()\n",
    "        inception_model_score.clear_fake()\n",
    "\n",
    "        #onload all GAN model to cpu and offload inception model to gpu\n",
    "        netG = netG.to(device)\n",
    "        netD = netD.to(device)\n",
    "        inception_model_score.model_to('cpu')\n",
    "        \n",
    "        \n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f, DivLoss : %.4f'\n",
    "          % (epoch, opt.niter, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2, loss_ds.item()))\n",
    "        \n",
    "        print(\"\\t\\tFID : %.4f, IS_f : %.4f, P : %.4f, R : %.4f, D : %.4f, C : %.4f\" \n",
    "              %(metrics['fid'], metrics['fake_is'], metrics['precision'], metrics['recall'], metrics['density'], metrics['coverage']))\n",
    "        \n",
    "       \n",
    "        vutils.save_image(real_cpu,\n",
    "                '%s/real_samples.png' % opt.outf,\n",
    "                normalize=True)\n",
    "        fake = netG(fixed_noise)\n",
    "        vutils.save_image(fake.detach(),\n",
    "                '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                normalize=True)\n",
    "        \n",
    "        fake2 = netG(fixed_noise + 1e-6)\n",
    "        vutils.save_image(fake2.detach(),\n",
    "                '%s/fake2_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                normalize=True)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\" : epoch,\n",
    "            \"Loss_D\": errD.item(),\n",
    "            \"Loss_G\": errG.item(),\n",
    "            \"D(real)\": D_x,\n",
    "            \"D(G(z))-before D train\": D_G_z1,\n",
    "            \"D(G(z))-after D train\": D_G_z2,\n",
    "            \"DivLoss\" : loss_ds.item(),\n",
    "            \"fid\" : metrics['fid'],\n",
    "            'fake_is':metrics['fake_is'],\n",
    "            \"precision\":metrics['precision'],\n",
    "            \"recall\":metrics['recall'],\n",
    "            \"density\":metrics['density'],\n",
    "            \"coverage\":metrics['coverage'],\n",
    "            \"G(z) \" : [wandb.Image(fake.detach().cpu().numpy(), caption='fixed z image')],\n",
    "            \"G(z + ) \" : [wandb.Image(fake2.detach().cpu().numpy(), caption='fixed z + 1e-6 image')],\n",
    "        })\n",
    "        \n",
    "        fake_np = vutils.make_grid(fake.detach().cpu(), nrow=32).permute(1,2,0).numpy()\n",
    "        fake2_np = vutils.make_grid(fake2.detach().cpu(), nrow=32).permute(1,2,0).numpy()\n",
    "        \n",
    "        wandb.log({\"examples\": [wandb.Image(numpy_array_or_pil, caption=\"Label\")]})\n",
    "\n",
    "        if opt.dry_run:\n",
    "            break\n",
    "    # do checkpointing\n",
    "    '''\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        vutils.save_image(fake.detach(),\n",
    "                '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                normalize=True)\n",
    "    '''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan2",
   "language": "python",
   "name": "stylegan2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
