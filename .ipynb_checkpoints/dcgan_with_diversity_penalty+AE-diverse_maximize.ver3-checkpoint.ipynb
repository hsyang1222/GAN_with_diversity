{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f89ed08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'cifar10', 'dataroot': '../dataset', 'workers': 2, 'batchSize': 2048, 'imageSize': 64, 'nz': 100, 'ngf': 64, 'ndf': 64, 'niter': 100, 'lr': 0.0002, 'beta1': 0.5, 'cuda': True, 'dry_run': False, 'ngpu': 1, 'netG': '', 'netD': '', 'netE': '', 'manualSeed': None, 'classes': None, 'outf': 'result_image', 'AEiter': 1, 'z_add': 0.8, 'lambda_diverse': 0.05, 'lambda_uniform': 1}\n",
      "Random Seed:  9888\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Encoder(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(512, 100, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Tanh()\n",
      "  )\n",
      ")\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../dataset\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=64, interpolation=bilinear)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "Using exist inception model info from : 0652f989f7f2a23771a3bf715c193582.pickle\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from dcgan import generative_model_score\n",
    "inception_model_score = generative_model_score.GenerativeModelScore()\n",
    "inception_model_score.lazy_mode(True)\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "    'dataset':'cifar10',\n",
    "    'dataroot':'../dataset',\n",
    "    'workers':2,\n",
    "    'batchSize':2048,\n",
    "    'imageSize':64,\n",
    "    'nz':100,\n",
    "    'ngf':64,\n",
    "    'ndf':64,\n",
    "    'niter':100,\n",
    "    'lr':0.0002,\n",
    "    'beta1':0.5,\n",
    "    'cuda':True,\n",
    "    'dry_run':False,\n",
    "    'ngpu':1,\n",
    "    'netG':'',\n",
    "    'netD':'',\n",
    "    'netE':'',\n",
    "    'manualSeed':None,\n",
    "    'classes':None,\n",
    "    'outf':'result_image',\n",
    "    'AEiter' : 1,\n",
    "    'z_add':0.8,\n",
    "    'lambda_diverse':0.05,\n",
    "    'lambda_uniform' : 1\n",
    "})\n",
    "\n",
    "\n",
    "#opt = parser.parse_args()\n",
    "opt = args\n",
    "print(opt)\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "  \n",
    "\n",
    "if opt.dataroot is None and str(opt.dataset).lower() != 'fake':\n",
    "    raise ValueError(\"`dataroot` parameter is required for dataset \\\"%s\\\"\" % opt.dataset)\n",
    "\n",
    "if opt.dataset in ['imagenet', 'folder', 'lfw']:\n",
    "    # folder dataset\n",
    "    dataset = dset.ImageFolder(root=opt.dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(opt.imageSize),\n",
    "                                   transforms.CenterCrop(opt.imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "    nc=3\n",
    "elif opt.dataset == 'lsun':\n",
    "    classes = [ c + '_train' for c in opt.classes.split(',')]\n",
    "    dataset = dset.LSUN(root=opt.dataroot, classes=classes,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize(opt.imageSize),\n",
    "                            transforms.CenterCrop(opt.imageSize),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ]))\n",
    "    nc=3\n",
    "elif opt.dataset == 'cifar10':\n",
    "    dataset = dset.CIFAR10(root=opt.dataroot, #download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(opt.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    nc=3\n",
    "\n",
    "elif opt.dataset == 'mnist':\n",
    "        dataset = dset.MNIST(root=opt.dataroot, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(opt.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,)),\n",
    "                           ]))\n",
    "        nc=1\n",
    "\n",
    "elif opt.dataset == 'fake':\n",
    "    dataset = dset.FakeData(image_size=(3, opt.imageSize, opt.imageSize),\n",
    "                            transform=transforms.ToTensor())\n",
    "    nc=3\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "\n",
    "device = torch.device(\"cuda:2\" if opt.cuda else \"cpu\")\n",
    "ngpu = int(opt.ngpu)\n",
    "nz = int(opt.nz)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "if opt.netG != '':\n",
    "    netG.load_state_dict(torch.load(opt.netG))\n",
    "print(netG)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 100, 4, 1, 0, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "        \n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "if opt.netD != '':\n",
    "    netD.load_state_dict(torch.load(opt.netD))\n",
    "print(netD)\n",
    "\n",
    "netE = Encoder(ngpu).to(device)\n",
    "netE.apply(weights_init)\n",
    "if opt.netE != '':\n",
    "    netE.load_state_dict(torch.load(opt.netE))\n",
    "print(netE)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerE = optim.Adam(netE.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "\n",
    "train_loader = dataloader\n",
    "print(train_loader.dataset)\n",
    "import hashlib\n",
    "real_images_info_file_name = inception_model_score.trainloaderinfo_to_hashedname(train_loader)\n",
    "if os.path.exists('../../inception_model_info/' + real_images_info_file_name) : \n",
    "    print(\"Using exist inception model info from :\", real_images_info_file_name)\n",
    "    inception_model_score.load_real_images_info('../../inception_model_info/' + real_images_info_file_name)\n",
    "else : \n",
    "    inception_model_score.model_to('cuda')\n",
    "\n",
    "    #put real image\n",
    "    for each_batch in train_loader : \n",
    "        X_train_batch = each_batch[0]\n",
    "        inception_model_score.put_real(X_train_batch)\n",
    "\n",
    "    #generate real images info\n",
    "    inception_model_score.lazy_forward(batch_size=64, device='cuda', real_forward=True)\n",
    "    inception_model_score.calculate_real_image_statistics()\n",
    "    #save real images info for next experiments\n",
    "    inception_model_score.save_real_images_info('../../inception_model_info/' + real_images_info_file_name)\n",
    "    print(\"Save inception model info to :\", real_images_info_file_name)\n",
    "    #offload inception_model\n",
    "    inception_model_score.model_to('cpu')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28d86fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhsyang1222\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">proud-meadow-19</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hsyang1222/GAN_with_diversit_maximize\" target=\"_blank\">https://wandb.ai/hsyang1222/GAN_with_diversit_maximize</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hsyang1222/GAN_with_diversit_maximize/runs/2cybwifo\" target=\"_blank\">https://wandb.ai/hsyang1222/GAN_with_diversit_maximize/runs/2cybwifo</a><br/>\n",
       "                Run data is saved locally in <code>/home/hsyang/workspace/20210306_gan/GAN_with_diversity/wandb/run-20210517_195327-2cybwifo</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='GAN_with_diversit_maximize', config=opt)\n",
    "config = wandb.config\n",
    "\n",
    "mse = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c999429",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ce41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2d22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = torch.rand(batch_size, nz, 1, 1, device=device)\n",
    "z2 = torch.rand(batch_size, nz, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1d86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_z1 = netG(z1)\n",
    "fake_z2 = netG(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae09803",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1dot5 = (z1 + z2) / 2\n",
    "fake_z1dot5 = netG(z_1dot5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd157ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_fakes_z1z2 = torch.mean(torch.abs(fake_z1 - fake_z2), dim=(1,2,3)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bf8e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_fake_z1dot5_z1 = torch.mean(torch.abs(fake_z1 - fake_z1dot5), dim=(1,2,3))\n",
    "diff_fake_z1dot5_z2 = torch.mean(torch.abs(fake_z2 - fake_z1dot5), dim=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8513586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_uniform_diff = mse(diff_fake_z1dot5_z1, diff_fakes_z1z2/2) + mse(diff_fake_z1dot5_z2, diff_fakes_z1z2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b106abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240e198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa8a0597",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 2; 11.91 GiB total capacity; 10.71 GiB already allocated; 18.94 MiB free; 11.18 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2eaae9a43e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# train with real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mreal_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         label = torch.full((batch_size,), real_label,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 2; 11.91 GiB total capacity; 10.71 GiB already allocated; 18.94 MiB free; 11.18 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# skip AE\n",
    "import tqdm\n",
    "for epoch in tqdm.tqdm(range(config.AEiter), desc=\"AE\"):\n",
    "    loss_sum = 0.\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_cuda = data[0].to(device)\n",
    "        batch_size = real_cuda.size(0)\n",
    "        \n",
    "        latent_vector = netE(real_cuda)\n",
    "        latent_4dim = latent_vector.view(batch_size,nz,1,1)\n",
    "        repaint = netG(latent_4dim)\n",
    "        \n",
    "        mse_loss = mse(repaint, real_cuda)\n",
    "        optimizerE.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        mse_loss.backward()\n",
    "        optimizerE.step()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        loss_sum += mse_loss.item()\n",
    "        \n",
    "del netE\n",
    "'''     \n",
    "for epoch in range(opt.niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label,\n",
    "                           dtype=real_cpu.dtype, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        z1 = torch.rand(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "        fake_z1 = netG(z1)\n",
    "        label.fill_(fake_label)\n",
    "        predict = netD(fake_z1.detach())\n",
    "\n",
    "        errD_fake = criterion(predict, label) \n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        if epoch % 10 == 0 :\n",
    "            inception_model_score.put_fake(fake_z1.detach().cpu())\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        z2 = torch.rand(batch_size, nz, 1, 1, device=device)\n",
    "        \n",
    "        fake_z1 = netG(z1)\n",
    "        fake_z2 = netG(z2)\n",
    "        \n",
    "        loss_maximize_div = torch.mean(torch.abs(fake_z1 - fake_z2)) # to maximize\n",
    "        diff_fakes_z1z2 = torch.mean(torch.abs(fake_z1 - fake_z2), dim=(1,2,3)).detach()\n",
    "        \n",
    "        z_1dot5 = ((z1 + z2) / 2).detach()\n",
    "        fake_z1dot5 = netG(z_1dot5)\n",
    "        \n",
    "        diff_fake_z1dot5_z1 = torch.mean(torch.abs(fake_z1.detach() - fake_z1dot5), dim=(1,2,3))\n",
    "        diff_fake_z1dot5_z2 = torch.mean(torch.abs(fake_z2.detach() - fake_z1dot5), dim=(1,2,3))\n",
    "        loss_uniform_diff = mse(diff_fake_z1dot5_z1, diff_fakes_z1z2/2) + mse(diff_fake_z1dot5_z2, diff_fakes_z1z2/2) # to uniform\n",
    "  \n",
    "        fake_z1 = netG(z1)\n",
    "        label.fill_(real_label)\n",
    "        predict = netD(fake_z1)\n",
    "        loss_label_g = criterion(predict, label)\n",
    "        \n",
    "        errG = loss_label_g - config.lambda_diverse * loss_maximize_div + config.lambda_uniform * loss_uniform_diff\n",
    "        errG.backward()\n",
    "        D_G_z2 = predict.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        netG = netG.to('cpu')\n",
    "        netD = netD.to('cpu')\n",
    "        inception_model_score.model_to(device)\n",
    "\n",
    "        #generate fake images info\n",
    "        inception_model_score.lazy_forward(batch_size=64, device=device, fake_forward=True)\n",
    "        inception_model_score.calculate_fake_image_statistics()\n",
    "        metrics = inception_model_score.calculate_generative_score()\n",
    "        inception_model_score.clear_fake()\n",
    "\n",
    "        #onload all GAN model to cpu and offload inception model to gpu\n",
    "        netG = netG.to(device)\n",
    "        netD = netD.to(device)\n",
    "        inception_model_score.model_to('cpu')\n",
    "        \n",
    "        \n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f, DivMaxLoss : %.4f, DivUniformLoss ; %.4f'\n",
    "          % (epoch, opt.niter, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2, loss_maximize_div.item(), loss_uniform_diff.item()))\n",
    "        \n",
    "        print(\"\\t\\tFID : %.4f, IS_f : %.4f, P : %.4f, R : %.4f, D : %.4f, C : %.4f\" \n",
    "              %(metrics['fid'], metrics['fake_is'], metrics['precision'], metrics['recall'], metrics['density'], metrics['coverage']))\n",
    "       \n",
    "        vutils.save_image(real_cpu,\n",
    "                '%s/real_samples.png' % opt.outf,\n",
    "                normalize=True)\n",
    "        fake_z1 = netG(fixed_noise)\n",
    "        vutils.save_image(fake.detach(),\n",
    "                '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                normalize=True)\n",
    "        \n",
    "        fake_z2 = netG(fixed_noise + config.z_add)\n",
    "        vutils.save_image(fake2.detach(),\n",
    "                '%s/fake2_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                normalize=True)\n",
    "        \n",
    "        fake_np = vutils.make_grid(fake_z1.detach().cpu(), nrow=32).permute(1,2,0).numpy()\n",
    "        fake2_np = vutils.make_grid(fake_z2.detach().cpu(), nrow=32).permute(1,2,0).numpy()\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\" : epoch,\n",
    "            \"Loss_D\": errD.item(),\n",
    "            \"Loss_G\": errG.item(),\n",
    "            \"D(real)\": D_x,\n",
    "            \"D(G(z))-before D train\": D_G_z1,\n",
    "            \"D(G(z))-after D train\": D_G_z2,\n",
    "            \"DivMaxLoss\" : loss_maximize_div.item(),\n",
    "            \"DivUniformLoss\" : loss_uniform_diff.item(),\n",
    "            \"fid\" : metrics['fid'],\n",
    "            'fake_is':metrics['fake_is'],\n",
    "            \"precision\":metrics['precision'],\n",
    "            \"recall\":metrics['recall'],\n",
    "            \"density\":metrics['density'],\n",
    "            \"coverage\":metrics['coverage'],\n",
    "            \"G(z) \" : [wandb.Image(fake_np, caption='fixed z image')],\n",
    "            \"G(z + div_add) \" : [wandb.Image(fake2_np, caption='fixed z + add image')],\n",
    "        })\n",
    "\n",
    "        if opt.dry_run:\n",
    "            break\n",
    "    # do checkpointing\n",
    "    '''\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        vutils.save_image(fake.detach(),\n",
    "                '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                normalize=True)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c02088bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21.0762, device='cuda:2', grad_fn=<BinaryCrossEntropyBackward>),\n",
       " tensor(45.4983, device='cuda:2', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_label_g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72640e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan2",
   "language": "python",
   "name": "stylegan2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
