{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'cifar10', 'dataroot': '../../dataset', 'workers': 2, 'batchSize': 64, 'imageSize': 64, 'nz': 100, 'ngf': 64, 'ndf': 64, 'niter': 25, 'lr': 0.0002, 'beta1': 0.5, 'cuda': False, 'dry_run': False, 'ngpu': 1, 'netG': '', 'netD': '', 'manualSeed': None, 'classes': None, 'outf': 'result_image'}\n",
      "Random Seed:  71\n",
      "WARNING: You have a CUDA device, so you should probably run with --cuda\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "    'dataset':'cifar10',\n",
    "    'dataroot':'../../dataset',\n",
    "    'workers':2,\n",
    "    'batchSize':64,\n",
    "    'imageSize':64,\n",
    "    'nz':100,\n",
    "    'ngf':64,\n",
    "    'ndf':64,\n",
    "    'niter':25,\n",
    "    'lr':0.0002,\n",
    "    'beta1':0.5,\n",
    "    'cuda':False,\n",
    "    'dry_run':False,\n",
    "    'ngpu':1,\n",
    "    'netG':'',\n",
    "    'netD':'',\n",
    "    'manualSeed':None,\n",
    "    'classes':None,\n",
    "    'outf':'result_image',\n",
    "})\n",
    "\n",
    "\n",
    "#opt = parser.parse_args()\n",
    "opt = args\n",
    "print(opt)\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "  \n",
    "\n",
    "if opt.dataroot is None and str(opt.dataset).lower() != 'fake':\n",
    "    raise ValueError(\"`dataroot` parameter is required for dataset \\\"%s\\\"\" % opt.dataset)\n",
    "\n",
    "if opt.dataset in ['imagenet', 'folder', 'lfw']:\n",
    "    # folder dataset\n",
    "    dataset = dset.ImageFolder(root=opt.dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(opt.imageSize),\n",
    "                                   transforms.CenterCrop(opt.imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "    nc=3\n",
    "elif opt.dataset == 'lsun':\n",
    "    classes = [ c + '_train' for c in opt.classes.split(',')]\n",
    "    dataset = dset.LSUN(root=opt.dataroot, classes=classes,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize(opt.imageSize),\n",
    "                            transforms.CenterCrop(opt.imageSize),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ]))\n",
    "    nc=3\n",
    "elif opt.dataset == 'cifar10':\n",
    "    dataset = dset.CIFAR10(root=opt.dataroot, #download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(opt.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    nc=3\n",
    "\n",
    "elif opt.dataset == 'mnist':\n",
    "        dataset = dset.MNIST(root=opt.dataroot, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(opt.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,)),\n",
    "                           ]))\n",
    "        nc=1\n",
    "\n",
    "elif opt.dataset == 'fake':\n",
    "    dataset = dset.FakeData(image_size=(3, opt.imageSize, opt.imageSize),\n",
    "                            transform=transforms.ToTensor())\n",
    "    nc=3\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "\n",
    "device = torch.device(\"cuda:2\" if opt.cuda else \"cpu\")\n",
    "ngpu = int(opt.ngpu)\n",
    "nz = int(opt.nz)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "if opt.netG != '':\n",
    "    netG.load_state_dict(torch.load(opt.netG))\n",
    "print(netG)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "if opt.netD != '':\n",
    "    netD.load_state_dict(torch.load(opt.netD))\n",
    "print(netD)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_activation_statistics(dataloader,model,batch_size=128, dims=2048, device=device):\n",
    "    model.eval()\n",
    "\n",
    "    pred_list = []\n",
    "    for data in dataloader : \n",
    "        batch=data[0].to(device)\n",
    "        pred = model(batch)[0]\n",
    "        pred_list.append(pred.detach().cpu())\n",
    "       \n",
    "    pred = torch.cat(pred_list)\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
    "    return act\n",
    "    \n",
    "def act_to_mu_sig(act):\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n",
    "    \n",
    "    \n",
    "from scipy import linalg\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    \n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1) +\n",
    "            np.trace(sigma2) - 2 * tr_covmean)\n",
    "            \n",
    "            \n",
    "def calculate_fretchet(images_real,images_fake,model):\n",
    "     mu_1,std_1=act_to_mu_sig(calculate_activation_statistics(images_real,model,cuda=True))\n",
    "     mu_2,std_2=act_to_mu_sig(calculate_activation_statistics(images_fake,model,cuda=True))\n",
    "    \n",
    "     \"\"\"get fretched distance\"\"\"\n",
    "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
    "     return fid_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid.inception import InceptionV3\n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "InceptionModel = InceptionV3([block_idx])\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "import prdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fakeDataloader = torchlist_to_dataloader(fake_data_list, 512,num_workers=num_workers)\n",
    "fake_act = calculate_activation_statistics(fakeDataloader,InceptionModel,device='cuda:0')\n",
    "fake_mu, fake_sigma=act_to_mu_sig(fake_act)\n",
    "fid_value_static = calculate_frechet_distance(real_mu, real_sigma, fake_mu, fake_sigma)\n",
    "\n",
    "real_pick = np.random.permutation(real_act)[:10000]\n",
    "fake_pick = np.random.permutation(fake_act)[:10000]\n",
    "prdc_metrics = prdc.compute_prdc(real_features=real_pick, fake_features=fake_pick, nearest_k=5)\n",
    "\n",
    "\n",
    "prdc_metrics['fid'] = float(fid_value_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][0/782] Loss_D: 1.8198 Loss_G: 5.6668 D(x): 0.4607 D(G(z)): 0.5231 / 0.0045\n",
      "[0/1][1/782] Loss_D: 0.9995 Loss_G: 5.2463 D(x): 0.6394 D(G(z)): 0.3397 / 0.0067\n",
      "[0/1][2/782] Loss_D: 0.8674 Loss_G: 5.9454 D(x): 0.7720 D(G(z)): 0.3599 / 0.0040\n",
      "[0/1][3/782] Loss_D: 0.7985 Loss_G: 6.2013 D(x): 0.7579 D(G(z)): 0.3117 / 0.0026\n",
      "[0/1][4/782] Loss_D: 0.7916 Loss_G: 6.6446 D(x): 0.7509 D(G(z)): 0.3070 / 0.0018\n",
      "[0/1][5/782] Loss_D: 0.8424 Loss_G: 7.8631 D(x): 0.7775 D(G(z)): 0.3824 / 0.0005\n",
      "[0/1][6/782] Loss_D: 0.5929 Loss_G: 7.0782 D(x): 0.7528 D(G(z)): 0.1815 / 0.0012\n",
      "[0/1][7/782] Loss_D: 0.9935 Loss_G: 8.0840 D(x): 0.7227 D(G(z)): 0.3609 / 0.0005\n",
      "[0/1][8/782] Loss_D: 0.7380 Loss_G: 7.5893 D(x): 0.7405 D(G(z)): 0.2106 / 0.0008\n",
      "[0/1][9/782] Loss_D: 0.7470 Loss_G: 9.3534 D(x): 0.8024 D(G(z)): 0.3266 / 0.0001\n",
      "[0/1][10/782] Loss_D: 0.3753 Loss_G: 8.1213 D(x): 0.8242 D(G(z)): 0.0958 / 0.0005\n",
      "[0/1][11/782] Loss_D: 0.7844 Loss_G: 10.0659 D(x): 0.7929 D(G(z)): 0.3132 / 0.0001\n",
      "[0/1][12/782] Loss_D: 0.2844 Loss_G: 8.9438 D(x): 0.8783 D(G(z)): 0.0906 / 0.0002\n",
      "[0/1][13/782] Loss_D: 0.6805 Loss_G: 12.1535 D(x): 0.8694 D(G(z)): 0.3227 / 0.0000\n",
      "[0/1][14/782] Loss_D: 0.2557 Loss_G: 8.9931 D(x): 0.8419 D(G(z)): 0.0168 / 0.0002\n",
      "[0/1][15/782] Loss_D: 0.3877 Loss_G: 10.6926 D(x): 0.9074 D(G(z)): 0.2253 / 0.0000\n",
      "[0/1][16/782] Loss_D: 0.2780 Loss_G: 9.2773 D(x): 0.8675 D(G(z)): 0.0641 / 0.0002\n",
      "[0/1][17/782] Loss_D: 0.4006 Loss_G: 11.4066 D(x): 0.8959 D(G(z)): 0.1949 / 0.0000\n",
      "[0/1][18/782] Loss_D: 0.2212 Loss_G: 9.7843 D(x): 0.8918 D(G(z)): 0.0736 / 0.0001\n",
      "[0/1][19/782] Loss_D: 0.7375 Loss_G: 15.3157 D(x): 0.8512 D(G(z)): 0.3493 / 0.0000\n",
      "[0/1][20/782] Loss_D: 0.1739 Loss_G: 12.1269 D(x): 0.8889 D(G(z)): 0.0021 / 0.0000\n",
      "[0/1][21/782] Loss_D: 0.1711 Loss_G: 8.0661 D(x): 0.9151 D(G(z)): 0.0687 / 0.0006\n",
      "[0/1][22/782] Loss_D: 1.0697 Loss_G: 19.6100 D(x): 0.9407 D(G(z)): 0.5877 / 0.0000\n",
      "[0/1][23/782] Loss_D: 0.3399 Loss_G: 20.4351 D(x): 0.7988 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][24/782] Loss_D: 0.1995 Loss_G: 14.8959 D(x): 0.8409 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][25/782] Loss_D: 0.0537 Loss_G: 6.0635 D(x): 0.9699 D(G(z)): 0.0211 / 0.0043\n",
      "[0/1][26/782] Loss_D: 4.3086 Loss_G: 21.4156 D(x): 0.9542 D(G(z)): 0.9765 / 0.0000\n",
      "[0/1][27/782] Loss_D: 0.3500 Loss_G: 23.9431 D(x): 0.7855 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][28/782] Loss_D: 0.4627 Loss_G: 20.2644 D(x): 0.7053 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][29/782] Loss_D: 0.1469 Loss_G: 11.5705 D(x): 0.8955 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][30/782] Loss_D: 0.4292 Loss_G: 12.2703 D(x): 0.9516 D(G(z)): 0.2780 / 0.0000\n",
      "[0/1][31/782] Loss_D: 0.1009 Loss_G: 10.3776 D(x): 0.9480 D(G(z)): 0.0385 / 0.0001\n",
      "[0/1][32/782] Loss_D: 0.4519 Loss_G: 16.5432 D(x): 0.9560 D(G(z)): 0.2957 / 0.0000\n",
      "[0/1][33/782] Loss_D: 0.3209 Loss_G: 14.0867 D(x): 0.8156 D(G(z)): 0.0005 / 0.0000\n",
      "[0/1][34/782] Loss_D: 0.0586 Loss_G: 7.7108 D(x): 0.9586 D(G(z)): 0.0146 / 0.0008\n",
      "[0/1][35/782] Loss_D: 1.6242 Loss_G: 23.3735 D(x): 0.9708 D(G(z)): 0.7505 / 0.0000\n",
      "[0/1][36/782] Loss_D: 0.4926 Loss_G: 25.6980 D(x): 0.7137 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][37/782] Loss_D: 0.2545 Loss_G: 22.5325 D(x): 0.8300 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][38/782] Loss_D: 0.0800 Loss_G: 15.0352 D(x): 0.9288 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][39/782] Loss_D: 0.0502 Loss_G: 5.7817 D(x): 0.9598 D(G(z)): 0.0076 / 0.0048\n",
      "[0/1][40/782] Loss_D: 2.3928 Loss_G: 23.7009 D(x): 0.9805 D(G(z)): 0.8697 / 0.0000\n",
      "[0/1][41/782] Loss_D: 0.4840 Loss_G: 27.0803 D(x): 0.7180 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][42/782] Loss_D: 0.0659 Loss_G: 25.7582 D(x): 0.9457 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][43/782] Loss_D: 0.0702 Loss_G: 21.9422 D(x): 0.9359 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][44/782] Loss_D: 0.0566 Loss_G: 15.7790 D(x): 0.9536 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][45/782] Loss_D: 0.0324 Loss_G: 9.2724 D(x): 0.9710 D(G(z)): 0.0002 / 0.0002\n",
      "[0/1][46/782] Loss_D: 0.1259 Loss_G: 5.0309 D(x): 0.9635 D(G(z)): 0.0567 / 0.0095\n",
      "[0/1][47/782] Loss_D: 0.7274 Loss_G: 16.2599 D(x): 0.9700 D(G(z)): 0.4721 / 0.0000\n",
      "[0/1][48/782] Loss_D: 0.2336 Loss_G: 17.9742 D(x): 0.8376 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][49/782] Loss_D: 0.1062 Loss_G: 16.6415 D(x): 0.9157 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][50/782] Loss_D: 0.1313 Loss_G: 11.5660 D(x): 0.9130 D(G(z)): 0.0001 / 0.0000\n",
      "[0/1][51/782] Loss_D: 0.0723 Loss_G: 5.9840 D(x): 0.9441 D(G(z)): 0.0085 / 0.0065\n",
      "[0/1][52/782] Loss_D: 0.5981 Loss_G: 12.3606 D(x): 0.9708 D(G(z)): 0.3775 / 0.0000\n",
      "[0/1][53/782] Loss_D: 0.4366 Loss_G: 11.0533 D(x): 0.7837 D(G(z)): 0.0011 / 0.0000\n",
      "[0/1][54/782] Loss_D: 0.1317 Loss_G: 7.5111 D(x): 0.8945 D(G(z)): 0.0045 / 0.0011\n",
      "[0/1][55/782] Loss_D: 0.4230 Loss_G: 10.9779 D(x): 0.9540 D(G(z)): 0.2634 / 0.0001\n",
      "[0/1][56/782] Loss_D: 0.1653 Loss_G: 9.1437 D(x): 0.9167 D(G(z)): 0.0304 / 0.0004\n",
      "[0/1][57/782] Loss_D: 0.1049 Loss_G: 8.0789 D(x): 0.9711 D(G(z)): 0.0653 / 0.0007\n",
      "[0/1][58/782] Loss_D: 0.6905 Loss_G: 17.0031 D(x): 0.9464 D(G(z)): 0.3979 / 0.0000\n",
      "[0/1][59/782] Loss_D: 0.2376 Loss_G: 17.5360 D(x): 0.8396 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][60/782] Loss_D: 0.2143 Loss_G: 13.1067 D(x): 0.8239 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][61/782] Loss_D: 0.0756 Loss_G: 7.2868 D(x): 0.9340 D(G(z)): 0.0037 / 0.0016\n",
      "[0/1][62/782] Loss_D: 0.3459 Loss_G: 10.0540 D(x): 0.9717 D(G(z)): 0.2318 / 0.0001\n",
      "[0/1][63/782] Loss_D: 0.1470 Loss_G: 8.7044 D(x): 0.9036 D(G(z)): 0.0092 / 0.0007\n",
      "[0/1][64/782] Loss_D: 0.0582 Loss_G: 6.8715 D(x): 0.9658 D(G(z)): 0.0202 / 0.0036\n",
      "[0/1][65/782] Loss_D: 0.2759 Loss_G: 6.9417 D(x): 0.9395 D(G(z)): 0.1257 / 0.0014\n",
      "[0/1][66/782] Loss_D: 0.1473 Loss_G: 7.4301 D(x): 0.9594 D(G(z)): 0.0958 / 0.0008\n",
      "[0/1][67/782] Loss_D: 0.1831 Loss_G: 6.7310 D(x): 0.9235 D(G(z)): 0.0748 / 0.0019\n",
      "[0/1][68/782] Loss_D: 0.2344 Loss_G: 6.1268 D(x): 0.8927 D(G(z)): 0.0945 / 0.0041\n",
      "[0/1][69/782] Loss_D: 0.3397 Loss_G: 7.1348 D(x): 0.8683 D(G(z)): 0.1477 / 0.0012\n",
      "[0/1][70/782] Loss_D: 0.1397 Loss_G: 6.7345 D(x): 0.9411 D(G(z)): 0.0674 / 0.0017\n",
      "[0/1][71/782] Loss_D: 0.1925 Loss_G: 6.3390 D(x): 0.9176 D(G(z)): 0.0866 / 0.0024\n",
      "[0/1][72/782] Loss_D: 0.3354 Loss_G: 6.7956 D(x): 0.8700 D(G(z)): 0.1390 / 0.0017\n",
      "[0/1][73/782] Loss_D: 0.1840 Loss_G: 7.0518 D(x): 0.9234 D(G(z)): 0.0914 / 0.0013\n",
      "[0/1][74/782] Loss_D: 0.3645 Loss_G: 4.2817 D(x): 0.8162 D(G(z)): 0.0649 / 0.0236\n",
      "[0/1][75/782] Loss_D: 0.8266 Loss_G: 16.2371 D(x): 0.9820 D(G(z)): 0.5106 / 0.0000\n",
      "[0/1][76/782] Loss_D: 3.1363 Loss_G: 9.4306 D(x): 0.1825 D(G(z)): 0.0000 / 0.0002\n",
      "[0/1][77/782] Loss_D: 0.5050 Loss_G: 3.4952 D(x): 0.7324 D(G(z)): 0.0026 / 0.0578\n",
      "[0/1][78/782] Loss_D: 0.9072 Loss_G: 9.2098 D(x): 0.9823 D(G(z)): 0.5235 / 0.0007\n",
      "[0/1][79/782] Loss_D: 0.5820 Loss_G: 7.2167 D(x): 0.7059 D(G(z)): 0.0091 / 0.0015\n",
      "[0/1][80/782] Loss_D: 0.0983 Loss_G: 5.0830 D(x): 0.9440 D(G(z)): 0.0216 / 0.0185\n",
      "[0/1][81/782] Loss_D: 0.2568 Loss_G: 5.3948 D(x): 0.9556 D(G(z)): 0.1443 / 0.0097\n",
      "[0/1][82/782] Loss_D: 0.3881 Loss_G: 6.5541 D(x): 0.9532 D(G(z)): 0.2491 / 0.0046\n",
      "[0/1][83/782] Loss_D: 0.2659 Loss_G: 5.6772 D(x): 0.8452 D(G(z)): 0.0560 / 0.0092\n",
      "[0/1][84/782] Loss_D: 0.3597 Loss_G: 4.3684 D(x): 0.8487 D(G(z)): 0.1208 / 0.0189\n",
      "[0/1][85/782] Loss_D: 0.5229 Loss_G: 5.6681 D(x): 0.8782 D(G(z)): 0.2325 / 0.0059\n",
      "[0/1][86/782] Loss_D: 0.3490 Loss_G: 4.7689 D(x): 0.8280 D(G(z)): 0.0762 / 0.0125\n",
      "[0/1][87/782] Loss_D: 0.3540 Loss_G: 4.8085 D(x): 0.8724 D(G(z)): 0.1619 / 0.0108\n",
      "[0/1][88/782] Loss_D: 0.2876 Loss_G: 4.9951 D(x): 0.8721 D(G(z)): 0.1056 / 0.0109\n",
      "[0/1][89/782] Loss_D: 0.4242 Loss_G: 7.1041 D(x): 0.9027 D(G(z)): 0.2377 / 0.0014\n",
      "[0/1][90/782] Loss_D: 0.2480 Loss_G: 5.2024 D(x): 0.8269 D(G(z)): 0.0272 / 0.0100\n",
      "[0/1][91/782] Loss_D: 0.3369 Loss_G: 6.3878 D(x): 0.9259 D(G(z)): 0.2106 / 0.0027\n",
      "[0/1][92/782] Loss_D: 0.3337 Loss_G: 4.9069 D(x): 0.8258 D(G(z)): 0.0736 / 0.0122\n",
      "[0/1][93/782] Loss_D: 0.3818 Loss_G: 7.0978 D(x): 0.9075 D(G(z)): 0.2297 / 0.0012\n",
      "[0/1][94/782] Loss_D: 0.3579 Loss_G: 4.3399 D(x): 0.7782 D(G(z)): 0.0432 / 0.0192\n",
      "[0/1][95/782] Loss_D: 0.5357 Loss_G: 9.1600 D(x): 0.9219 D(G(z)): 0.3303 / 0.0007\n",
      "[0/1][96/782] Loss_D: 0.9351 Loss_G: 3.5841 D(x): 0.5472 D(G(z)): 0.0106 / 0.0502\n",
      "[0/1][97/782] Loss_D: 1.1028 Loss_G: 12.6303 D(x): 0.9567 D(G(z)): 0.6184 / 0.0000\n",
      "[0/1][98/782] Loss_D: 2.0878 Loss_G: 8.1333 D(x): 0.3480 D(G(z)): 0.0004 / 0.0046\n",
      "[0/1][99/782] Loss_D: 0.3624 Loss_G: 3.9941 D(x): 0.8424 D(G(z)): 0.0789 / 0.0366\n",
      "[0/1][100/782] Loss_D: 1.3775 Loss_G: 10.4498 D(x): 0.9600 D(G(z)): 0.6342 / 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][101/782] Loss_D: 1.4458 Loss_G: 6.6411 D(x): 0.4071 D(G(z)): 0.0017 / 0.0022\n",
      "[0/1][102/782] Loss_D: 0.0918 Loss_G: 3.6876 D(x): 0.9508 D(G(z)): 0.0358 / 0.0566\n",
      "[0/1][103/782] Loss_D: 0.6709 Loss_G: 6.5954 D(x): 0.9643 D(G(z)): 0.3843 / 0.0052\n",
      "[0/1][104/782] Loss_D: 0.2321 Loss_G: 6.4633 D(x): 0.8667 D(G(z)): 0.0443 / 0.0066\n",
      "[0/1][105/782] Loss_D: 0.3123 Loss_G: 4.9794 D(x): 0.8345 D(G(z)): 0.0469 / 0.0255\n",
      "[0/1][106/782] Loss_D: 0.2917 Loss_G: 4.2273 D(x): 0.9195 D(G(z)): 0.1633 / 0.0242\n",
      "[0/1][107/782] Loss_D: 0.4516 Loss_G: 4.9444 D(x): 0.8908 D(G(z)): 0.2279 / 0.0176\n",
      "[0/1][108/782] Loss_D: 0.5439 Loss_G: 3.8339 D(x): 0.7454 D(G(z)): 0.1400 / 0.0397\n",
      "[0/1][109/782] Loss_D: 0.8533 Loss_G: 8.2772 D(x): 0.8813 D(G(z)): 0.4419 / 0.0005\n",
      "[0/1][110/782] Loss_D: 0.8947 Loss_G: 4.0663 D(x): 0.5382 D(G(z)): 0.0093 / 0.0270\n",
      "[0/1][111/782] Loss_D: 0.7381 Loss_G: 7.2695 D(x): 0.8966 D(G(z)): 0.4103 / 0.0034\n",
      "[0/1][112/782] Loss_D: 0.1661 Loss_G: 6.8422 D(x): 0.8960 D(G(z)): 0.0411 / 0.0029\n",
      "[0/1][113/782] Loss_D: 0.3197 Loss_G: 3.7316 D(x): 0.8082 D(G(z)): 0.0644 / 0.0384\n",
      "[0/1][114/782] Loss_D: 0.7364 Loss_G: 7.7034 D(x): 0.9203 D(G(z)): 0.4266 / 0.0012\n",
      "[0/1][115/782] Loss_D: 0.5030 Loss_G: 5.4378 D(x): 0.6960 D(G(z)): 0.0155 / 0.0089\n",
      "[0/1][116/782] Loss_D: 0.2034 Loss_G: 4.3730 D(x): 0.9408 D(G(z)): 0.1103 / 0.0219\n",
      "[0/1][117/782] Loss_D: 0.4645 Loss_G: 7.6024 D(x): 0.9513 D(G(z)): 0.2948 / 0.0028\n",
      "[0/1][118/782] Loss_D: 0.6127 Loss_G: 5.4510 D(x): 0.6915 D(G(z)): 0.0177 / 0.0095\n",
      "[0/1][119/782] Loss_D: 0.1796 Loss_G: 3.8435 D(x): 0.9366 D(G(z)): 0.0808 / 0.0321\n",
      "[0/1][120/782] Loss_D: 0.4222 Loss_G: 5.6459 D(x): 0.9542 D(G(z)): 0.2736 / 0.0048\n",
      "[0/1][121/782] Loss_D: 0.3108 Loss_G: 4.3582 D(x): 0.8342 D(G(z)): 0.0603 / 0.0155\n",
      "[0/1][122/782] Loss_D: 0.3703 Loss_G: 5.9091 D(x): 0.9227 D(G(z)): 0.2274 / 0.0058\n",
      "[0/1][123/782] Loss_D: 0.4670 Loss_G: 3.9906 D(x): 0.7524 D(G(z)): 0.0720 / 0.0249\n",
      "[0/1][124/782] Loss_D: 0.5584 Loss_G: 7.7690 D(x): 0.8905 D(G(z)): 0.3262 / 0.0016\n",
      "[0/1][125/782] Loss_D: 0.4877 Loss_G: 4.5919 D(x): 0.7067 D(G(z)): 0.0240 / 0.0476\n",
      "[0/1][126/782] Loss_D: 0.6961 Loss_G: 9.1752 D(x): 0.9310 D(G(z)): 0.3240 / 0.0023\n",
      "[0/1][127/782] Loss_D: 0.8812 Loss_G: 5.3286 D(x): 0.6700 D(G(z)): 0.0332 / 0.0225\n",
      "[0/1][128/782] Loss_D: 0.4294 Loss_G: 5.1596 D(x): 0.8874 D(G(z)): 0.2263 / 0.0210\n",
      "[0/1][129/782] Loss_D: 0.5753 Loss_G: 7.0945 D(x): 0.8407 D(G(z)): 0.2249 / 0.0042\n",
      "[0/1][130/782] Loss_D: 0.5025 Loss_G: 4.0040 D(x): 0.7485 D(G(z)): 0.0773 / 0.0401\n",
      "[0/1][131/782] Loss_D: 0.7353 Loss_G: 8.3362 D(x): 0.9099 D(G(z)): 0.3683 / 0.0014\n",
      "[0/1][132/782] Loss_D: 0.7013 Loss_G: 5.4902 D(x): 0.6558 D(G(z)): 0.0174 / 0.0110\n",
      "[0/1][133/782] Loss_D: 0.2323 Loss_G: 4.0133 D(x): 0.9200 D(G(z)): 0.1080 / 0.0333\n",
      "[0/1][134/782] Loss_D: 0.7405 Loss_G: 9.6870 D(x): 0.9614 D(G(z)): 0.4478 / 0.0002\n",
      "[0/1][135/782] Loss_D: 1.5600 Loss_G: 4.2160 D(x): 0.3888 D(G(z)): 0.0032 / 0.0408\n",
      "[0/1][136/782] Loss_D: 0.6652 Loss_G: 7.4678 D(x): 0.9721 D(G(z)): 0.3707 / 0.0020\n",
      "[0/1][137/782] Loss_D: 0.5526 Loss_G: 4.6544 D(x): 0.7011 D(G(z)): 0.0333 / 0.0242\n",
      "[0/1][138/782] Loss_D: 0.4470 Loss_G: 5.9813 D(x): 0.9178 D(G(z)): 0.2697 / 0.0041\n",
      "[0/1][139/782] Loss_D: 0.3016 Loss_G: 6.1151 D(x): 0.8700 D(G(z)): 0.1118 / 0.0039\n",
      "[0/1][140/782] Loss_D: 0.4814 Loss_G: 4.7940 D(x): 0.7851 D(G(z)): 0.1321 / 0.0124\n",
      "[0/1][141/782] Loss_D: 0.3952 Loss_G: 6.8875 D(x): 0.9063 D(G(z)): 0.2269 / 0.0037\n",
      "[0/1][142/782] Loss_D: 0.7365 Loss_G: 2.8221 D(x): 0.6804 D(G(z)): 0.0789 / 0.1180\n",
      "[0/1][143/782] Loss_D: 1.9644 Loss_G: 13.3959 D(x): 0.9584 D(G(z)): 0.7313 / 0.0000\n",
      "[0/1][144/782] Loss_D: 3.1647 Loss_G: 8.2848 D(x): 0.2272 D(G(z)): 0.0004 / 0.0031\n",
      "[0/1][145/782] Loss_D: 0.9079 Loss_G: 2.5466 D(x): 0.6226 D(G(z)): 0.0270 / 0.1805\n",
      "[0/1][146/782] Loss_D: 1.7123 Loss_G: 6.0956 D(x): 0.9706 D(G(z)): 0.6668 / 0.0123\n",
      "[0/1][147/782] Loss_D: 0.2427 Loss_G: 6.7954 D(x): 0.8848 D(G(z)): 0.0711 / 0.0016\n",
      "[0/1][148/782] Loss_D: 0.5332 Loss_G: 4.1749 D(x): 0.6761 D(G(z)): 0.0170 / 0.0232\n",
      "[0/1][149/782] Loss_D: 0.3802 Loss_G: 3.4747 D(x): 0.8916 D(G(z)): 0.2046 / 0.0421\n",
      "[0/1][150/782] Loss_D: 0.5008 Loss_G: 5.3967 D(x): 0.9309 D(G(z)): 0.3221 / 0.0061\n",
      "[0/1][151/782] Loss_D: 0.1875 Loss_G: 5.4568 D(x): 0.8997 D(G(z)): 0.0649 / 0.0071\n",
      "[0/1][152/782] Loss_D: 0.5174 Loss_G: 3.0418 D(x): 0.7457 D(G(z)): 0.0767 / 0.0675\n",
      "[0/1][153/782] Loss_D: 0.7541 Loss_G: 6.2324 D(x): 0.9174 D(G(z)): 0.4359 / 0.0027\n",
      "[0/1][154/782] Loss_D: 0.5330 Loss_G: 4.3959 D(x): 0.6694 D(G(z)): 0.0194 / 0.0176\n",
      "[0/1][155/782] Loss_D: 0.3837 Loss_G: 4.2052 D(x): 0.8745 D(G(z)): 0.1889 / 0.0214\n",
      "[0/1][156/782] Loss_D: 0.4533 Loss_G: 5.7014 D(x): 0.8735 D(G(z)): 0.2171 / 0.0054\n",
      "[0/1][157/782] Loss_D: 0.3799 Loss_G: 4.0573 D(x): 0.7750 D(G(z)): 0.0577 / 0.0246\n",
      "[0/1][158/782] Loss_D: 0.4005 Loss_G: 6.3862 D(x): 0.9250 D(G(z)): 0.2510 / 0.0025\n",
      "[0/1][159/782] Loss_D: 0.3625 Loss_G: 4.2086 D(x): 0.7732 D(G(z)): 0.0415 / 0.0233\n",
      "[0/1][160/782] Loss_D: 0.4196 Loss_G: 6.6157 D(x): 0.9322 D(G(z)): 0.2524 / 0.0021\n",
      "[0/1][161/782] Loss_D: 0.5458 Loss_G: 3.3151 D(x): 0.6720 D(G(z)): 0.0433 / 0.0586\n",
      "[0/1][162/782] Loss_D: 0.6948 Loss_G: 8.5680 D(x): 0.9262 D(G(z)): 0.3849 / 0.0005\n",
      "[0/1][163/782] Loss_D: 0.4460 Loss_G: 6.7218 D(x): 0.7064 D(G(z)): 0.0043 / 0.0033\n",
      "[0/1][164/782] Loss_D: 0.1910 Loss_G: 3.6201 D(x): 0.8973 D(G(z)): 0.0275 / 0.0389\n",
      "[0/1][165/782] Loss_D: 0.5173 Loss_G: 7.6402 D(x): 0.9705 D(G(z)): 0.3432 / 0.0010\n",
      "[0/1][166/782] Loss_D: 0.3786 Loss_G: 5.3757 D(x): 0.7675 D(G(z)): 0.0306 / 0.0075\n",
      "[0/1][167/782] Loss_D: 0.2499 Loss_G: 5.0482 D(x): 0.8969 D(G(z)): 0.0992 / 0.0134\n",
      "[0/1][168/782] Loss_D: 0.3488 Loss_G: 6.5263 D(x): 0.8895 D(G(z)): 0.1806 / 0.0022\n",
      "[0/1][169/782] Loss_D: 0.4482 Loss_G: 3.6364 D(x): 0.7537 D(G(z)): 0.0231 / 0.0418\n",
      "[0/1][170/782] Loss_D: 0.4388 Loss_G: 7.7405 D(x): 0.9612 D(G(z)): 0.2937 / 0.0010\n",
      "[0/1][171/782] Loss_D: 0.1690 Loss_G: 6.4317 D(x): 0.8886 D(G(z)): 0.0152 / 0.0039\n",
      "[0/1][172/782] Loss_D: 0.2252 Loss_G: 3.9613 D(x): 0.8498 D(G(z)): 0.0251 / 0.0259\n",
      "[0/1][173/782] Loss_D: 0.4473 Loss_G: 8.7216 D(x): 0.9868 D(G(z)): 0.3263 / 0.0004\n",
      "[0/1][174/782] Loss_D: 0.3855 Loss_G: 6.9749 D(x): 0.7497 D(G(z)): 0.0040 / 0.0019\n",
      "[0/1][175/782] Loss_D: 0.0843 Loss_G: 4.7585 D(x): 0.9571 D(G(z)): 0.0352 / 0.0168\n",
      "[0/1][176/782] Loss_D: 0.2302 Loss_G: 6.1106 D(x): 0.9703 D(G(z)): 0.1703 / 0.0030\n",
      "[0/1][177/782] Loss_D: 0.1928 Loss_G: 5.6068 D(x): 0.8960 D(G(z)): 0.0353 / 0.0057\n",
      "[0/1][178/782] Loss_D: 0.2725 Loss_G: 4.4313 D(x): 0.8627 D(G(z)): 0.0889 / 0.0150\n",
      "[0/1][179/782] Loss_D: 0.2267 Loss_G: 6.4963 D(x): 0.9586 D(G(z)): 0.1574 / 0.0019\n",
      "[0/1][180/782] Loss_D: 0.1319 Loss_G: 6.2685 D(x): 0.9224 D(G(z)): 0.0446 / 0.0024\n",
      "[0/1][181/782] Loss_D: 0.1927 Loss_G: 4.7670 D(x): 0.8933 D(G(z)): 0.0347 / 0.0104\n",
      "[0/1][182/782] Loss_D: 0.2125 Loss_G: 7.5632 D(x): 0.9733 D(G(z)): 0.1606 / 0.0007\n",
      "[0/1][183/782] Loss_D: 0.1932 Loss_G: 6.2284 D(x): 0.8589 D(G(z)): 0.0195 / 0.0028\n",
      "[0/1][184/782] Loss_D: 0.1210 Loss_G: 5.7455 D(x): 0.9577 D(G(z)): 0.0705 / 0.0040\n",
      "[0/1][185/782] Loss_D: 0.2009 Loss_G: 6.1047 D(x): 0.9057 D(G(z)): 0.0850 / 0.0027\n",
      "[0/1][186/782] Loss_D: 0.1802 Loss_G: 7.5102 D(x): 0.9434 D(G(z)): 0.1065 / 0.0008\n",
      "[0/1][187/782] Loss_D: 0.1204 Loss_G: 6.4491 D(x): 0.9141 D(G(z)): 0.0237 / 0.0021\n",
      "[0/1][188/782] Loss_D: 0.1051 Loss_G: 5.4048 D(x): 0.9381 D(G(z)): 0.0354 / 0.0056\n",
      "[0/1][189/782] Loss_D: 0.3786 Loss_G: 10.9196 D(x): 0.9509 D(G(z)): 0.2507 / 0.0000\n",
      "[0/1][190/782] Loss_D: 0.1214 Loss_G: 10.9607 D(x): 0.8990 D(G(z)): 0.0006 / 0.0000\n",
      "[0/1][191/782] Loss_D: 0.1242 Loss_G: 8.4137 D(x): 0.9031 D(G(z)): 0.0005 / 0.0003\n",
      "[0/1][192/782] Loss_D: 0.0710 Loss_G: 5.1121 D(x): 0.9612 D(G(z)): 0.0131 / 0.0086\n",
      "[0/1][193/782] Loss_D: 0.2965 Loss_G: 9.1128 D(x): 0.9810 D(G(z)): 0.2261 / 0.0002\n",
      "[0/1][194/782] Loss_D: 0.1265 Loss_G: 8.6712 D(x): 0.9052 D(G(z)): 0.0042 / 0.0003\n",
      "[0/1][195/782] Loss_D: 0.0673 Loss_G: 6.4789 D(x): 0.9453 D(G(z)): 0.0079 / 0.0025\n",
      "[0/1][196/782] Loss_D: 0.0970 Loss_G: 5.3286 D(x): 0.9617 D(G(z)): 0.0530 / 0.0064\n",
      "[0/1][197/782] Loss_D: 0.3114 Loss_G: 10.1592 D(x): 0.9706 D(G(z)): 0.2272 / 0.0001\n",
      "[0/1][198/782] Loss_D: 0.2368 Loss_G: 9.0461 D(x): 0.8211 D(G(z)): 0.0014 / 0.0004\n",
      "[0/1][199/782] Loss_D: 0.1106 Loss_G: 5.8834 D(x): 0.9351 D(G(z)): 0.0105 / 0.0052\n",
      "[0/1][200/782] Loss_D: 0.1922 Loss_G: 7.6110 D(x): 0.9775 D(G(z)): 0.1474 / 0.0007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][201/782] Loss_D: 0.1019 Loss_G: 7.5799 D(x): 0.9521 D(G(z)): 0.0466 / 0.0008\n",
      "[0/1][202/782] Loss_D: 0.2884 Loss_G: 6.6736 D(x): 0.8567 D(G(z)): 0.0859 / 0.0019\n",
      "[0/1][203/782] Loss_D: 0.2430 Loss_G: 11.3796 D(x): 0.9637 D(G(z)): 0.1710 / 0.0000\n",
      "[0/1][204/782] Loss_D: 0.0454 Loss_G: 10.6418 D(x): 0.9595 D(G(z)): 0.0018 / 0.0000\n",
      "[0/1][205/782] Loss_D: 0.1921 Loss_G: 6.5065 D(x): 0.8837 D(G(z)): 0.0039 / 0.0021\n",
      "[0/1][206/782] Loss_D: 0.4421 Loss_G: 15.7772 D(x): 0.9922 D(G(z)): 0.3237 / 0.0000\n",
      "[0/1][207/782] Loss_D: 0.6255 Loss_G: 14.5500 D(x): 0.6617 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][208/782] Loss_D: 0.1008 Loss_G: 9.8340 D(x): 0.9535 D(G(z)): 0.0002 / 0.0001\n",
      "[0/1][209/782] Loss_D: 0.0584 Loss_G: 5.3255 D(x): 0.9885 D(G(z)): 0.0443 / 0.0084\n",
      "[0/1][210/782] Loss_D: 0.9511 Loss_G: 18.3838 D(x): 0.9966 D(G(z)): 0.5436 / 0.0000\n",
      "[0/1][211/782] Loss_D: 0.6022 Loss_G: 18.3160 D(x): 0.7193 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][212/782] Loss_D: 0.3401 Loss_G: 12.5566 D(x): 0.8300 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][213/782] Loss_D: 0.0965 Loss_G: 5.8314 D(x): 0.9599 D(G(z)): 0.0375 / 0.0096\n",
      "[0/1][214/782] Loss_D: 1.3690 Loss_G: 16.7324 D(x): 0.9829 D(G(z)): 0.6070 / 0.0000\n",
      "[0/1][215/782] Loss_D: 0.5677 Loss_G: 16.3780 D(x): 0.7547 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][216/782] Loss_D: 0.1812 Loss_G: 14.4639 D(x): 0.8680 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][217/782] Loss_D: 0.1409 Loss_G: 10.0854 D(x): 0.9193 D(G(z)): 0.0003 / 0.0004\n",
      "[0/1][218/782] Loss_D: 0.0881 Loss_G: 6.4249 D(x): 0.9900 D(G(z)): 0.0597 / 0.0093\n",
      "[0/1][219/782] Loss_D: 0.5160 Loss_G: 13.0173 D(x): 0.9833 D(G(z)): 0.2923 / 0.0000\n",
      "[0/1][220/782] Loss_D: 0.3266 Loss_G: 12.8521 D(x): 0.8151 D(G(z)): 0.0002 / 0.0000\n",
      "[0/1][221/782] Loss_D: 0.0353 Loss_G: 10.5266 D(x): 0.9691 D(G(z)): 0.0008 / 0.0005\n",
      "[0/1][222/782] Loss_D: 0.0211 Loss_G: 7.4926 D(x): 0.9836 D(G(z)): 0.0041 / 0.0033\n",
      "[0/1][223/782] Loss_D: 0.2353 Loss_G: 5.7911 D(x): 0.9232 D(G(z)): 0.1074 / 0.0103\n",
      "[0/1][224/782] Loss_D: 0.4122 Loss_G: 11.2485 D(x): 0.9726 D(G(z)): 0.2601 / 0.0001\n",
      "[0/1][225/782] Loss_D: 0.5682 Loss_G: 6.5243 D(x): 0.6558 D(G(z)): 0.0005 / 0.0052\n",
      "[0/1][226/782] Loss_D: 0.7515 Loss_G: 13.1827 D(x): 0.9713 D(G(z)): 0.3208 / 0.0000\n",
      "[0/1][227/782] Loss_D: 0.7098 Loss_G: 9.5842 D(x): 0.6498 D(G(z)): 0.0006 / 0.0003\n",
      "[0/1][228/782] Loss_D: 0.0953 Loss_G: 5.3203 D(x): 0.9419 D(G(z)): 0.0236 / 0.0147\n",
      "[0/1][229/782] Loss_D: 0.7814 Loss_G: 14.9196 D(x): 0.9726 D(G(z)): 0.4159 / 0.0000\n",
      "[0/1][230/782] Loss_D: 3.6514 Loss_G: 6.3456 D(x): 0.0853 D(G(z)): 0.0000 / 0.0651\n",
      "[0/1][231/782] Loss_D: 0.6596 Loss_G: 6.9644 D(x): 0.9707 D(G(z)): 0.2251 / 0.0055\n",
      "[0/1][232/782] Loss_D: 0.4174 Loss_G: 6.2505 D(x): 0.8854 D(G(z)): 0.1331 / 0.0160\n",
      "[0/1][233/782] Loss_D: 0.3659 Loss_G: 5.9434 D(x): 0.8746 D(G(z)): 0.1573 / 0.0120\n",
      "[0/1][234/782] Loss_D: 0.2422 Loss_G: 5.5291 D(x): 0.9116 D(G(z)): 0.1072 / 0.0101\n",
      "[0/1][235/782] Loss_D: 0.3801 Loss_G: 4.6614 D(x): 0.8551 D(G(z)): 0.1518 / 0.0184\n",
      "[0/1][236/782] Loss_D: 0.4065 Loss_G: 6.5044 D(x): 0.9363 D(G(z)): 0.2341 / 0.0033\n",
      "[0/1][237/782] Loss_D: 0.1859 Loss_G: 5.8303 D(x): 0.9025 D(G(z)): 0.0599 / 0.0056\n",
      "[0/1][238/782] Loss_D: 0.5506 Loss_G: 3.6393 D(x): 0.7822 D(G(z)): 0.0766 / 0.0553\n",
      "[0/1][239/782] Loss_D: 0.6407 Loss_G: 8.2383 D(x): 0.9323 D(G(z)): 0.3597 / 0.0009\n",
      "[0/1][240/782] Loss_D: 0.3413 Loss_G: 6.1527 D(x): 0.7659 D(G(z)): 0.0110 / 0.0071\n",
      "[0/1][241/782] Loss_D: 0.2727 Loss_G: 4.6356 D(x): 0.9002 D(G(z)): 0.1083 / 0.0241\n",
      "[0/1][242/782] Loss_D: 0.4630 Loss_G: 8.6757 D(x): 0.9629 D(G(z)): 0.2666 / 0.0008\n",
      "[0/1][243/782] Loss_D: 0.4577 Loss_G: 5.8356 D(x): 0.7741 D(G(z)): 0.0418 / 0.0109\n",
      "[0/1][244/782] Loss_D: 0.5130 Loss_G: 7.7926 D(x): 0.8833 D(G(z)): 0.2225 / 0.0013\n",
      "[0/1][245/782] Loss_D: 0.5078 Loss_G: 5.6006 D(x): 0.7636 D(G(z)): 0.0619 / 0.0115\n",
      "[0/1][246/782] Loss_D: 0.5144 Loss_G: 7.2155 D(x): 0.8335 D(G(z)): 0.1968 / 0.0016\n",
      "[0/1][247/782] Loss_D: 0.3078 Loss_G: 5.6491 D(x): 0.8431 D(G(z)): 0.0723 / 0.0066\n",
      "[0/1][248/782] Loss_D: 0.1726 Loss_G: 7.0559 D(x): 0.9596 D(G(z)): 0.1130 / 0.0014\n",
      "[0/1][249/782] Loss_D: 0.2164 Loss_G: 6.9233 D(x): 0.9051 D(G(z)): 0.0777 / 0.0014\n",
      "[0/1][250/782] Loss_D: 0.2390 Loss_G: 4.8854 D(x): 0.8919 D(G(z)): 0.0363 / 0.0113\n",
      "[0/1][251/782] Loss_D: 0.4754 Loss_G: 10.9524 D(x): 0.9155 D(G(z)): 0.2782 / 0.0001\n",
      "[0/1][252/782] Loss_D: 1.0976 Loss_G: 3.9553 D(x): 0.4713 D(G(z)): 0.0008 / 0.0500\n",
      "[0/1][253/782] Loss_D: 0.7925 Loss_G: 13.2364 D(x): 0.9947 D(G(z)): 0.4377 / 0.0000\n",
      "[0/1][254/782] Loss_D: 0.5820 Loss_G: 11.3418 D(x): 0.6758 D(G(z)): 0.0004 / 0.0002\n",
      "[0/1][255/782] Loss_D: 0.0448 Loss_G: 8.1518 D(x): 0.9613 D(G(z)): 0.0023 / 0.0037\n",
      "[0/1][256/782] Loss_D: 0.2084 Loss_G: 6.9931 D(x): 0.9706 D(G(z)): 0.1129 / 0.0019\n",
      "[0/1][257/782] Loss_D: 0.0809 Loss_G: 6.1613 D(x): 0.9780 D(G(z)): 0.0535 / 0.0030\n",
      "[0/1][258/782] Loss_D: 0.1656 Loss_G: 5.5972 D(x): 0.9517 D(G(z)): 0.0763 / 0.0060\n",
      "[0/1][259/782] Loss_D: 0.2041 Loss_G: 5.3321 D(x): 0.9253 D(G(z)): 0.0817 / 0.0068\n",
      "[0/1][260/782] Loss_D: 0.2196 Loss_G: 6.7714 D(x): 0.9402 D(G(z)): 0.1272 / 0.0018\n",
      "[0/1][261/782] Loss_D: 0.3524 Loss_G: 4.2107 D(x): 0.7936 D(G(z)): 0.0273 / 0.0188\n",
      "[0/1][262/782] Loss_D: 0.3692 Loss_G: 8.1681 D(x): 0.9539 D(G(z)): 0.2410 / 0.0007\n",
      "[0/1][263/782] Loss_D: 0.1373 Loss_G: 8.0068 D(x): 0.9041 D(G(z)): 0.0059 / 0.0013\n",
      "[0/1][264/782] Loss_D: 0.1183 Loss_G: 6.0519 D(x): 0.9226 D(G(z)): 0.0123 / 0.0082\n",
      "[0/1][265/782] Loss_D: 0.1935 Loss_G: 4.6559 D(x): 0.9379 D(G(z)): 0.1067 / 0.0151\n",
      "[0/1][266/782] Loss_D: 0.5462 Loss_G: 8.5091 D(x): 0.9311 D(G(z)): 0.3222 / 0.0004\n",
      "[0/1][267/782] Loss_D: 0.9854 Loss_G: 5.2366 D(x): 0.6873 D(G(z)): 0.0188 / 0.0117\n",
      "[0/1][268/782] Loss_D: 0.3369 Loss_G: 6.0664 D(x): 0.9351 D(G(z)): 0.1924 / 0.0042\n",
      "[0/1][269/782] Loss_D: 0.3693 Loss_G: 7.3033 D(x): 0.8950 D(G(z)): 0.1836 / 0.0013\n",
      "[0/1][270/782] Loss_D: 0.3773 Loss_G: 5.2804 D(x): 0.7889 D(G(z)): 0.0160 / 0.0072\n",
      "[0/1][271/782] Loss_D: 0.2036 Loss_G: 6.3589 D(x): 0.9738 D(G(z)): 0.1516 / 0.0029\n",
      "[0/1][272/782] Loss_D: 0.1900 Loss_G: 6.1777 D(x): 0.8866 D(G(z)): 0.0503 / 0.0031\n",
      "[0/1][273/782] Loss_D: 0.1569 Loss_G: 4.9379 D(x): 0.9042 D(G(z)): 0.0450 / 0.0094\n",
      "[0/1][274/782] Loss_D: 0.2101 Loss_G: 6.4562 D(x): 0.9331 D(G(z)): 0.1161 / 0.0025\n",
      "[0/1][275/782] Loss_D: 0.1568 Loss_G: 5.8935 D(x): 0.9067 D(G(z)): 0.0307 / 0.0053\n",
      "[0/1][276/782] Loss_D: 0.1508 Loss_G: 5.2117 D(x): 0.9193 D(G(z)): 0.0550 / 0.0110\n",
      "[0/1][277/782] Loss_D: 0.1591 Loss_G: 5.9835 D(x): 0.9327 D(G(z)): 0.0696 / 0.0051\n",
      "[0/1][278/782] Loss_D: 0.1600 Loss_G: 5.9218 D(x): 0.9118 D(G(z)): 0.0476 / 0.0050\n",
      "[0/1][279/782] Loss_D: 0.1470 Loss_G: 6.3206 D(x): 0.9382 D(G(z)): 0.0669 / 0.0041\n",
      "[0/1][280/782] Loss_D: 0.1077 Loss_G: 5.5434 D(x): 0.9271 D(G(z)): 0.0204 / 0.0057\n",
      "[0/1][281/782] Loss_D: 0.1471 Loss_G: 7.9576 D(x): 0.9704 D(G(z)): 0.1038 / 0.0007\n",
      "[0/1][282/782] Loss_D: 0.4416 Loss_G: 4.3924 D(x): 0.7337 D(G(z)): 0.0098 / 0.0385\n",
      "[0/1][283/782] Loss_D: 0.2902 Loss_G: 8.0785 D(x): 0.9498 D(G(z)): 0.1730 / 0.0007\n",
      "[0/1][284/782] Loss_D: 0.1023 Loss_G: 7.9723 D(x): 0.9363 D(G(z)): 0.0247 / 0.0016\n",
      "[0/1][285/782] Loss_D: 0.2686 Loss_G: 3.8530 D(x): 0.8443 D(G(z)): 0.0088 / 0.0375\n",
      "[0/1][286/782] Loss_D: 0.4020 Loss_G: 9.9008 D(x): 0.9596 D(G(z)): 0.2485 / 0.0002\n",
      "[0/1][287/782] Loss_D: 0.0775 Loss_G: 9.6049 D(x): 0.9364 D(G(z)): 0.0024 / 0.0003\n",
      "[0/1][288/782] Loss_D: 0.1391 Loss_G: 6.7398 D(x): 0.8959 D(G(z)): 0.0025 / 0.0027\n",
      "[0/1][289/782] Loss_D: 0.1896 Loss_G: 2.9746 D(x): 0.8856 D(G(z)): 0.0429 / 0.0656\n",
      "[0/1][290/782] Loss_D: 0.8458 Loss_G: 13.5846 D(x): 0.9542 D(G(z)): 0.4810 / 0.0000\n",
      "[0/1][291/782] Loss_D: 1.2473 Loss_G: 10.1240 D(x): 0.4903 D(G(z)): 0.0001 / 0.0024\n",
      "[0/1][292/782] Loss_D: 0.0929 Loss_G: 7.1657 D(x): 0.9263 D(G(z)): 0.0064 / 0.0205\n",
      "[0/1][293/782] Loss_D: 0.2419 Loss_G: 6.8461 D(x): 0.9722 D(G(z)): 0.1518 / 0.0014\n",
      "[0/1][294/782] Loss_D: 0.0638 Loss_G: 5.9075 D(x): 0.9578 D(G(z)): 0.0171 / 0.0063\n",
      "[0/1][295/782] Loss_D: 0.1122 Loss_G: 4.8266 D(x): 0.9486 D(G(z)): 0.0473 / 0.0124\n",
      "[0/1][296/782] Loss_D: 0.1652 Loss_G: 5.4710 D(x): 0.9659 D(G(z)): 0.1139 / 0.0064\n",
      "[0/1][297/782] Loss_D: 0.1907 Loss_G: 5.2006 D(x): 0.9137 D(G(z)): 0.0575 / 0.0096\n",
      "[0/1][298/782] Loss_D: 0.2612 Loss_G: 4.4851 D(x): 0.8864 D(G(z)): 0.0822 / 0.0156\n",
      "[0/1][299/782] Loss_D: 0.2226 Loss_G: 4.7761 D(x): 0.9080 D(G(z)): 0.1024 / 0.0124\n",
      "[0/1][300/782] Loss_D: 0.1199 Loss_G: 5.3362 D(x): 0.9588 D(G(z)): 0.0713 / 0.0066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][301/782] Loss_D: 0.1235 Loss_G: 5.0213 D(x): 0.9499 D(G(z)): 0.0612 / 0.0095\n",
      "[0/1][302/782] Loss_D: 0.2690 Loss_G: 4.6818 D(x): 0.8852 D(G(z)): 0.1105 / 0.0149\n",
      "[0/1][303/782] Loss_D: 0.2194 Loss_G: 5.1393 D(x): 0.9189 D(G(z)): 0.1093 / 0.0098\n",
      "[0/1][304/782] Loss_D: 0.2922 Loss_G: 3.6590 D(x): 0.8388 D(G(z)): 0.0655 / 0.0430\n",
      "[0/1][305/782] Loss_D: 0.4706 Loss_G: 7.8205 D(x): 0.9254 D(G(z)): 0.2683 / 0.0015\n",
      "[0/1][306/782] Loss_D: 0.6327 Loss_G: 4.7442 D(x): 0.6537 D(G(z)): 0.0035 / 0.0182\n",
      "[0/1][307/782] Loss_D: 0.1057 Loss_G: 3.9421 D(x): 0.9765 D(G(z)): 0.0734 / 0.0318\n",
      "[0/1][308/782] Loss_D: 0.3387 Loss_G: 7.2334 D(x): 0.9570 D(G(z)): 0.2304 / 0.0013\n",
      "[0/1][309/782] Loss_D: 0.1919 Loss_G: 6.6068 D(x): 0.8726 D(G(z)): 0.0078 / 0.0036\n",
      "[0/1][310/782] Loss_D: 0.2173 Loss_G: 4.0303 D(x): 0.8586 D(G(z)): 0.0273 / 0.0534\n",
      "[0/1][311/782] Loss_D: 0.2996 Loss_G: 5.3626 D(x): 0.9551 D(G(z)): 0.1945 / 0.0099\n",
      "[0/1][312/782] Loss_D: 0.2791 Loss_G: 4.5355 D(x): 0.8957 D(G(z)): 0.0741 / 0.0184\n",
      "[0/1][313/782] Loss_D: 0.2069 Loss_G: 4.2898 D(x): 0.9258 D(G(z)): 0.0734 / 0.0233\n",
      "[0/1][314/782] Loss_D: 0.2841 Loss_G: 4.8398 D(x): 0.9201 D(G(z)): 0.1591 / 0.0131\n",
      "[0/1][315/782] Loss_D: 0.4161 Loss_G: 3.3775 D(x): 0.7877 D(G(z)): 0.0774 / 0.0535\n",
      "[0/1][316/782] Loss_D: 0.3645 Loss_G: 4.8148 D(x): 0.9145 D(G(z)): 0.2055 / 0.0127\n",
      "[0/1][317/782] Loss_D: 0.2265 Loss_G: 4.8033 D(x): 0.8947 D(G(z)): 0.0790 / 0.0134\n",
      "[0/1][318/782] Loss_D: 0.2332 Loss_G: 4.0842 D(x): 0.8870 D(G(z)): 0.0866 / 0.0251\n",
      "[0/1][319/782] Loss_D: 0.2368 Loss_G: 4.8009 D(x): 0.9320 D(G(z)): 0.1370 / 0.0137\n",
      "[0/1][320/782] Loss_D: 0.3514 Loss_G: 4.1592 D(x): 0.8473 D(G(z)): 0.1096 / 0.0247\n",
      "[0/1][321/782] Loss_D: 0.3571 Loss_G: 4.6806 D(x): 0.8713 D(G(z)): 0.1434 / 0.0145\n",
      "[0/1][322/782] Loss_D: 0.2406 Loss_G: 5.3937 D(x): 0.9147 D(G(z)): 0.1209 / 0.0075\n",
      "[0/1][323/782] Loss_D: 0.3545 Loss_G: 3.0470 D(x): 0.7793 D(G(z)): 0.0503 / 0.1070\n",
      "[0/1][324/782] Loss_D: 0.6728 Loss_G: 9.4850 D(x): 0.9189 D(G(z)): 0.3595 / 0.0003\n",
      "[0/1][325/782] Loss_D: 1.0492 Loss_G: 4.8284 D(x): 0.5834 D(G(z)): 0.0017 / 0.0333\n",
      "[0/1][326/782] Loss_D: 0.2183 Loss_G: 4.5267 D(x): 0.9536 D(G(z)): 0.1355 / 0.0249\n",
      "[0/1][327/782] Loss_D: 0.3359 Loss_G: 6.1047 D(x): 0.9123 D(G(z)): 0.1547 / 0.0061\n",
      "[0/1][328/782] Loss_D: 0.2109 Loss_G: 5.9401 D(x): 0.9180 D(G(z)): 0.0823 / 0.0066\n",
      "[0/1][329/782] Loss_D: 0.2916 Loss_G: 4.2956 D(x): 0.8686 D(G(z)): 0.0712 / 0.0350\n",
      "[0/1][330/782] Loss_D: 0.3576 Loss_G: 5.5891 D(x): 0.9378 D(G(z)): 0.1925 / 0.0105\n",
      "[0/1][331/782] Loss_D: 0.4811 Loss_G: 3.9946 D(x): 0.7763 D(G(z)): 0.0886 / 0.0520\n",
      "[0/1][332/782] Loss_D: 0.3259 Loss_G: 5.9050 D(x): 0.9512 D(G(z)): 0.2025 / 0.0068\n",
      "[0/1][333/782] Loss_D: 0.3279 Loss_G: 4.7037 D(x): 0.8445 D(G(z)): 0.1045 / 0.0205\n",
      "[0/1][334/782] Loss_D: 0.4510 Loss_G: 3.2473 D(x): 0.7911 D(G(z)): 0.0984 / 0.0755\n",
      "[0/1][335/782] Loss_D: 0.5594 Loss_G: 6.0952 D(x): 0.8789 D(G(z)): 0.2848 / 0.0047\n",
      "[0/1][336/782] Loss_D: 0.3846 Loss_G: 3.5641 D(x): 0.7744 D(G(z)): 0.0349 / 0.0542\n",
      "[0/1][337/782] Loss_D: 0.3233 Loss_G: 4.1201 D(x): 0.8976 D(G(z)): 0.1731 / 0.0300\n",
      "[0/1][338/782] Loss_D: 0.1952 Loss_G: 4.4865 D(x): 0.9227 D(G(z)): 0.0903 / 0.0254\n",
      "[0/1][339/782] Loss_D: 0.3573 Loss_G: 5.5290 D(x): 0.9097 D(G(z)): 0.1948 / 0.0068\n",
      "[0/1][340/782] Loss_D: 0.9523 Loss_G: 0.8767 D(x): 0.5455 D(G(z)): 0.0531 / 0.4920\n",
      "[0/1][341/782] Loss_D: 1.6798 Loss_G: 10.0304 D(x): 0.9804 D(G(z)): 0.6924 / 0.0004\n",
      "[0/1][342/782] Loss_D: 2.2019 Loss_G: 4.7024 D(x): 0.2684 D(G(z)): 0.0054 / 0.0820\n",
      "[0/1][343/782] Loss_D: 0.7524 Loss_G: 2.6795 D(x): 0.7869 D(G(z)): 0.2128 / 0.1762\n",
      "[0/1][344/782] Loss_D: 0.9658 Loss_G: 5.8336 D(x): 0.9315 D(G(z)): 0.4295 / 0.0112\n",
      "[0/1][345/782] Loss_D: 1.1769 Loss_G: 2.0891 D(x): 0.4873 D(G(z)): 0.0275 / 0.2119\n",
      "[0/1][346/782] Loss_D: 0.8797 Loss_G: 4.1978 D(x): 0.8861 D(G(z)): 0.4041 / 0.0527\n",
      "[0/1][347/782] Loss_D: 0.8408 Loss_G: 3.1380 D(x): 0.6864 D(G(z)): 0.1952 / 0.0862\n",
      "[0/1][348/782] Loss_D: 0.6570 Loss_G: 2.3476 D(x): 0.7560 D(G(z)): 0.1189 / 0.1491\n",
      "[0/1][349/782] Loss_D: 0.7811 Loss_G: 5.7186 D(x): 0.9380 D(G(z)): 0.4241 / 0.0069\n",
      "[0/1][350/782] Loss_D: 1.0940 Loss_G: 2.0346 D(x): 0.4776 D(G(z)): 0.0191 / 0.2369\n",
      "[0/1][351/782] Loss_D: 0.7267 Loss_G: 3.6391 D(x): 0.9394 D(G(z)): 0.4093 / 0.0442\n",
      "[0/1][352/782] Loss_D: 0.5829 Loss_G: 4.0775 D(x): 0.8281 D(G(z)): 0.2299 / 0.0339\n",
      "[0/1][353/782] Loss_D: 0.8471 Loss_G: 1.6895 D(x): 0.6034 D(G(z)): 0.1221 / 0.2649\n",
      "[0/1][354/782] Loss_D: 0.9649 Loss_G: 5.6662 D(x): 0.9255 D(G(z)): 0.5411 / 0.0069\n",
      "[0/1][355/782] Loss_D: 2.1143 Loss_G: 0.5752 D(x): 0.2382 D(G(z)): 0.0508 / 0.6244\n",
      "[0/1][356/782] Loss_D: 2.0029 Loss_G: 6.2790 D(x): 0.9615 D(G(z)): 0.7835 / 0.0068\n",
      "[0/1][357/782] Loss_D: 1.2573 Loss_G: 3.8630 D(x): 0.4267 D(G(z)): 0.0258 / 0.0596\n",
      "[0/1][358/782] Loss_D: 0.4790 Loss_G: 2.4533 D(x): 0.7682 D(G(z)): 0.1147 / 0.1541\n",
      "[0/1][359/782] Loss_D: 0.6425 Loss_G: 4.2000 D(x): 0.9489 D(G(z)): 0.3466 / 0.0247\n",
      "[0/1][360/782] Loss_D: 0.5953 Loss_G: 3.6184 D(x): 0.7580 D(G(z)): 0.1514 / 0.0439\n",
      "[0/1][361/782] Loss_D: 0.4675 Loss_G: 3.7208 D(x): 0.8543 D(G(z)): 0.2038 / 0.0359\n",
      "[0/1][362/782] Loss_D: 0.5794 Loss_G: 2.8016 D(x): 0.7476 D(G(z)): 0.1763 / 0.0858\n",
      "[0/1][363/782] Loss_D: 0.7270 Loss_G: 4.0919 D(x): 0.7884 D(G(z)): 0.3097 / 0.0234\n",
      "[0/1][364/782] Loss_D: 0.6111 Loss_G: 3.3029 D(x): 0.7393 D(G(z)): 0.1585 / 0.0521\n",
      "[0/1][365/782] Loss_D: 0.5020 Loss_G: 3.3420 D(x): 0.7923 D(G(z)): 0.1818 / 0.0553\n",
      "[0/1][366/782] Loss_D: 0.6408 Loss_G: 4.5077 D(x): 0.8148 D(G(z)): 0.2766 / 0.0192\n",
      "[0/1][367/782] Loss_D: 0.4319 Loss_G: 3.3743 D(x): 0.7557 D(G(z)): 0.0870 / 0.0486\n",
      "[0/1][368/782] Loss_D: 0.5345 Loss_G: 3.7593 D(x): 0.8220 D(G(z)): 0.2350 / 0.0351\n",
      "[0/1][369/782] Loss_D: 0.4956 Loss_G: 5.3086 D(x): 0.8357 D(G(z)): 0.2217 / 0.0091\n",
      "[0/1][370/782] Loss_D: 0.5604 Loss_G: 2.9186 D(x): 0.7408 D(G(z)): 0.0822 / 0.1135\n",
      "[0/1][371/782] Loss_D: 0.9135 Loss_G: 6.0070 D(x): 0.8165 D(G(z)): 0.4154 / 0.0130\n",
      "[0/1][372/782] Loss_D: 0.9509 Loss_G: 3.1307 D(x): 0.5842 D(G(z)): 0.0952 / 0.0781\n",
      "[0/1][373/782] Loss_D: 0.5888 Loss_G: 6.1977 D(x): 0.9240 D(G(z)): 0.3573 / 0.0062\n",
      "[0/1][374/782] Loss_D: 0.6598 Loss_G: 3.3321 D(x): 0.6652 D(G(z)): 0.0437 / 0.0815\n",
      "[0/1][375/782] Loss_D: 0.6932 Loss_G: 7.1548 D(x): 0.9398 D(G(z)): 0.3940 / 0.0016\n",
      "[0/1][376/782] Loss_D: 0.9836 Loss_G: 1.4148 D(x): 0.4621 D(G(z)): 0.0175 / 0.3432\n",
      "[0/1][377/782] Loss_D: 1.5532 Loss_G: 8.7538 D(x): 0.9388 D(G(z)): 0.6847 / 0.0006\n",
      "[0/1][378/782] Loss_D: 2.1021 Loss_G: 1.8548 D(x): 0.2714 D(G(z)): 0.0044 / 0.2972\n",
      "[0/1][379/782] Loss_D: 1.4054 Loss_G: 6.6437 D(x): 0.9393 D(G(z)): 0.5568 / 0.0059\n",
      "[0/1][380/782] Loss_D: 1.3822 Loss_G: 2.2800 D(x): 0.4339 D(G(z)): 0.0442 / 0.2447\n",
      "[0/1][381/782] Loss_D: 1.3816 Loss_G: 5.1444 D(x): 0.9047 D(G(z)): 0.5726 / 0.0157\n",
      "[0/1][382/782] Loss_D: 1.0522 Loss_G: 2.9979 D(x): 0.5105 D(G(z)): 0.0671 / 0.0904\n",
      "[0/1][383/782] Loss_D: 1.0591 Loss_G: 3.1523 D(x): 0.7238 D(G(z)): 0.4235 / 0.0899\n",
      "[0/1][384/782] Loss_D: 0.8967 Loss_G: 2.4135 D(x): 0.6261 D(G(z)): 0.1851 / 0.1247\n",
      "[0/1][385/782] Loss_D: 0.6002 Loss_G: 4.3695 D(x): 0.9042 D(G(z)): 0.3542 / 0.0175\n",
      "[0/1][386/782] Loss_D: 0.4810 Loss_G: 3.4626 D(x): 0.7219 D(G(z)): 0.0693 / 0.0451\n",
      "[0/1][387/782] Loss_D: 0.5440 Loss_G: 3.7624 D(x): 0.8514 D(G(z)): 0.2671 / 0.0436\n",
      "[0/1][388/782] Loss_D: 0.6050 Loss_G: 3.4689 D(x): 0.7660 D(G(z)): 0.2009 / 0.0494\n",
      "[0/1][389/782] Loss_D: 0.7195 Loss_G: 2.9500 D(x): 0.7365 D(G(z)): 0.2364 / 0.0874\n",
      "[0/1][390/782] Loss_D: 1.1275 Loss_G: 6.2391 D(x): 0.8002 D(G(z)): 0.4692 / 0.0085\n",
      "[0/1][391/782] Loss_D: 1.1499 Loss_G: 3.4876 D(x): 0.4709 D(G(z)): 0.0249 / 0.0777\n",
      "[0/1][392/782] Loss_D: 0.3272 Loss_G: 3.2150 D(x): 0.9344 D(G(z)): 0.2015 / 0.0668\n",
      "[0/1][393/782] Loss_D: 0.3858 Loss_G: 4.1875 D(x): 0.8910 D(G(z)): 0.1973 / 0.0237\n",
      "[0/1][394/782] Loss_D: 0.3166 Loss_G: 3.6899 D(x): 0.8468 D(G(z)): 0.1103 / 0.0445\n",
      "[0/1][395/782] Loss_D: 0.4568 Loss_G: 4.3211 D(x): 0.8864 D(G(z)): 0.2255 / 0.0236\n",
      "[0/1][396/782] Loss_D: 0.3241 Loss_G: 4.2469 D(x): 0.8795 D(G(z)): 0.1397 / 0.0235\n",
      "[0/1][397/782] Loss_D: 0.4473 Loss_G: 2.8757 D(x): 0.7539 D(G(z)): 0.0580 / 0.0897\n",
      "[0/1][398/782] Loss_D: 0.6400 Loss_G: 4.9064 D(x): 0.9164 D(G(z)): 0.3614 / 0.0129\n",
      "[0/1][399/782] Loss_D: 0.6158 Loss_G: 3.2325 D(x): 0.6729 D(G(z)): 0.0740 / 0.0616\n",
      "[0/1][400/782] Loss_D: 0.5465 Loss_G: 2.4728 D(x): 0.7458 D(G(z)): 0.1604 / 0.1455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][401/782] Loss_D: 0.5516 Loss_G: 6.0585 D(x): 0.9699 D(G(z)): 0.3721 / 0.0041\n",
      "[0/1][402/782] Loss_D: 0.2644 Loss_G: 6.0395 D(x): 0.8031 D(G(z)): 0.0096 / 0.0042\n",
      "[0/1][403/782] Loss_D: 0.1937 Loss_G: 3.7527 D(x): 0.8603 D(G(z)): 0.0277 / 0.0411\n",
      "[0/1][404/782] Loss_D: 0.3494 Loss_G: 4.3054 D(x): 0.9118 D(G(z)): 0.1836 / 0.0181\n",
      "[0/1][405/782] Loss_D: 0.3077 Loss_G: 5.8385 D(x): 0.8967 D(G(z)): 0.1468 / 0.0068\n",
      "[0/1][406/782] Loss_D: 0.6502 Loss_G: 1.9192 D(x): 0.6844 D(G(z)): 0.0430 / 0.2064\n",
      "[0/1][407/782] Loss_D: 1.4579 Loss_G: 10.5224 D(x): 0.9502 D(G(z)): 0.6670 / 0.0001\n",
      "[0/1][408/782] Loss_D: 2.2102 Loss_G: 2.6331 D(x): 0.2911 D(G(z)): 0.0012 / 0.1416\n",
      "[0/1][409/782] Loss_D: 2.0689 Loss_G: 9.6652 D(x): 0.9218 D(G(z)): 0.7429 / 0.0003\n",
      "[0/1][410/782] Loss_D: 3.1011 Loss_G: 2.6085 D(x): 0.1330 D(G(z)): 0.0074 / 0.1193\n",
      "[0/1][411/782] Loss_D: 0.9621 Loss_G: 4.0535 D(x): 0.8242 D(G(z)): 0.4397 / 0.0265\n",
      "[0/1][412/782] Loss_D: 0.4750 Loss_G: 4.2111 D(x): 0.7856 D(G(z)): 0.1505 / 0.0236\n",
      "[0/1][413/782] Loss_D: 0.5349 Loss_G: 3.0377 D(x): 0.7540 D(G(z)): 0.1223 / 0.0710\n",
      "[0/1][414/782] Loss_D: 0.4752 Loss_G: 5.2748 D(x): 0.9438 D(G(z)): 0.3061 / 0.0085\n",
      "[0/1][415/782] Loss_D: 0.3629 Loss_G: 4.4668 D(x): 0.8374 D(G(z)): 0.0905 / 0.0183\n",
      "[0/1][416/782] Loss_D: 0.2863 Loss_G: 4.4015 D(x): 0.9301 D(G(z)): 0.1630 / 0.0209\n",
      "[0/1][417/782] Loss_D: 0.6843 Loss_G: 2.7534 D(x): 0.7169 D(G(z)): 0.1618 / 0.0854\n",
      "[0/1][418/782] Loss_D: 0.8667 Loss_G: 5.8849 D(x): 0.8712 D(G(z)): 0.4565 / 0.0048\n",
      "[0/1][419/782] Loss_D: 0.6186 Loss_G: 3.8217 D(x): 0.6309 D(G(z)): 0.0311 / 0.0342\n",
      "[0/1][420/782] Loss_D: 0.4097 Loss_G: 3.0722 D(x): 0.8316 D(G(z)): 0.1544 / 0.0624\n",
      "[0/1][421/782] Loss_D: 0.4317 Loss_G: 5.6964 D(x): 0.9530 D(G(z)): 0.2927 / 0.0055\n",
      "[0/1][422/782] Loss_D: 0.4439 Loss_G: 4.1285 D(x): 0.7205 D(G(z)): 0.0291 / 0.0269\n",
      "[0/1][423/782] Loss_D: 0.3234 Loss_G: 2.9345 D(x): 0.8460 D(G(z)): 0.0969 / 0.0734\n",
      "[0/1][424/782] Loss_D: 0.5911 Loss_G: 6.9300 D(x): 0.9484 D(G(z)): 0.3854 / 0.0021\n",
      "[0/1][425/782] Loss_D: 0.4491 Loss_G: 4.8386 D(x): 0.6938 D(G(z)): 0.0150 / 0.0160\n",
      "[0/1][426/782] Loss_D: 0.2308 Loss_G: 2.7032 D(x): 0.8405 D(G(z)): 0.0443 / 0.0978\n",
      "[0/1][427/782] Loss_D: 0.7701 Loss_G: 5.2441 D(x): 0.9301 D(G(z)): 0.4413 / 0.0086\n",
      "[0/1][428/782] Loss_D: 0.7153 Loss_G: 3.0184 D(x): 0.6301 D(G(z)): 0.0751 / 0.0829\n",
      "[0/1][429/782] Loss_D: 0.6961 Loss_G: 4.9853 D(x): 0.8872 D(G(z)): 0.3772 / 0.0129\n",
      "[0/1][430/782] Loss_D: 0.4030 Loss_G: 3.9618 D(x): 0.7641 D(G(z)): 0.0639 / 0.0295\n",
      "[0/1][431/782] Loss_D: 0.3967 Loss_G: 3.4742 D(x): 0.8594 D(G(z)): 0.1791 / 0.0473\n",
      "[0/1][432/782] Loss_D: 0.4664 Loss_G: 3.2401 D(x): 0.8068 D(G(z)): 0.1771 / 0.0542\n",
      "[0/1][433/782] Loss_D: 0.4515 Loss_G: 5.1546 D(x): 0.8996 D(G(z)): 0.2623 / 0.0079\n",
      "[0/1][434/782] Loss_D: 0.2330 Loss_G: 4.4569 D(x): 0.8293 D(G(z)): 0.0221 / 0.0180\n",
      "[0/1][435/782] Loss_D: 0.1702 Loss_G: 3.3163 D(x): 0.9119 D(G(z)): 0.0677 / 0.0497\n",
      "[0/1][436/782] Loss_D: 0.3154 Loss_G: 4.0284 D(x): 0.8973 D(G(z)): 0.1678 / 0.0256\n",
      "[0/1][437/782] Loss_D: 0.3677 Loss_G: 6.2661 D(x): 0.9395 D(G(z)): 0.2463 / 0.0027\n",
      "[0/1][438/782] Loss_D: 0.7969 Loss_G: 2.3859 D(x): 0.5378 D(G(z)): 0.0216 / 0.1273\n",
      "[0/1][439/782] Loss_D: 0.5264 Loss_G: 4.8811 D(x): 0.9606 D(G(z)): 0.3345 / 0.0145\n",
      "[0/1][440/782] Loss_D: 0.3236 Loss_G: 4.4951 D(x): 0.8199 D(G(z)): 0.0866 / 0.0213\n",
      "[0/1][441/782] Loss_D: 0.1943 Loss_G: 3.7452 D(x): 0.9015 D(G(z)): 0.0653 / 0.0290\n",
      "[0/1][442/782] Loss_D: 0.1499 Loss_G: 4.1581 D(x): 0.9696 D(G(z)): 0.1090 / 0.0212\n",
      "[0/1][443/782] Loss_D: 0.2644 Loss_G: 4.0174 D(x): 0.8920 D(G(z)): 0.1155 / 0.0254\n",
      "[0/1][444/782] Loss_D: 0.2521 Loss_G: 3.9983 D(x): 0.8914 D(G(z)): 0.1160 / 0.0285\n",
      "[0/1][445/782] Loss_D: 0.4631 Loss_G: 4.8474 D(x): 0.8542 D(G(z)): 0.2278 / 0.0103\n",
      "[0/1][446/782] Loss_D: 0.2869 Loss_G: 3.5781 D(x): 0.8051 D(G(z)): 0.0359 / 0.0365\n",
      "[0/1][447/782] Loss_D: 0.3767 Loss_G: 5.4443 D(x): 0.9129 D(G(z)): 0.2258 / 0.0069\n",
      "[0/1][448/782] Loss_D: 0.3127 Loss_G: 4.0126 D(x): 0.7944 D(G(z)): 0.0502 / 0.0243\n",
      "[0/1][449/782] Loss_D: 0.3525 Loss_G: 4.1222 D(x): 0.8529 D(G(z)): 0.1585 / 0.0208\n",
      "[0/1][450/782] Loss_D: 0.2163 Loss_G: 6.3554 D(x): 0.9479 D(G(z)): 0.1376 / 0.0029\n",
      "[0/1][451/782] Loss_D: 0.2019 Loss_G: 4.5391 D(x): 0.8767 D(G(z)): 0.0461 / 0.0165\n",
      "[0/1][452/782] Loss_D: 0.3003 Loss_G: 4.0276 D(x): 0.8751 D(G(z)): 0.1205 / 0.0267\n",
      "[0/1][453/782] Loss_D: 0.3885 Loss_G: 8.1212 D(x): 0.9694 D(G(z)): 0.2598 / 0.0005\n",
      "[0/1][454/782] Loss_D: 1.0667 Loss_G: 1.7392 D(x): 0.4736 D(G(z)): 0.0050 / 0.2322\n",
      "[0/1][455/782] Loss_D: 1.4325 Loss_G: 9.9127 D(x): 0.9871 D(G(z)): 0.6691 / 0.0002\n",
      "[0/1][456/782] Loss_D: 1.2548 Loss_G: 6.3150 D(x): 0.3950 D(G(z)): 0.0013 / 0.0065\n",
      "[0/1][457/782] Loss_D: 0.1094 Loss_G: 4.1106 D(x): 0.9600 D(G(z)): 0.0613 / 0.0383\n",
      "[0/1][458/782] Loss_D: 0.4879 Loss_G: 8.5243 D(x): 0.9800 D(G(z)): 0.3100 / 0.0004\n",
      "[0/1][459/782] Loss_D: 1.5284 Loss_G: 0.9969 D(x): 0.3185 D(G(z)): 0.0045 / 0.4724\n",
      "[0/1][460/782] Loss_D: 2.2143 Loss_G: 11.1898 D(x): 0.9988 D(G(z)): 0.7991 / 0.0001\n",
      "[0/1][461/782] Loss_D: 1.4791 Loss_G: 5.5105 D(x): 0.3412 D(G(z)): 0.0004 / 0.0350\n",
      "[0/1][462/782] Loss_D: 0.5788 Loss_G: 5.4285 D(x): 0.9151 D(G(z)): 0.2537 / 0.0157\n",
      "[0/1][463/782] Loss_D: 0.3381 Loss_G: 7.2427 D(x): 0.9591 D(G(z)): 0.2168 / 0.0017\n",
      "[0/1][464/782] Loss_D: 0.2840 Loss_G: 5.4539 D(x): 0.8219 D(G(z)): 0.0400 / 0.0087\n",
      "[0/1][465/782] Loss_D: 0.8043 Loss_G: 5.4119 D(x): 0.8034 D(G(z)): 0.3283 / 0.0100\n",
      "[0/1][466/782] Loss_D: 0.6901 Loss_G: 3.3044 D(x): 0.6753 D(G(z)): 0.1004 / 0.0715\n",
      "[0/1][467/782] Loss_D: 0.9496 Loss_G: 8.1242 D(x): 0.8792 D(G(z)): 0.4767 / 0.0008\n",
      "[0/1][468/782] Loss_D: 1.0526 Loss_G: 4.6120 D(x): 0.4644 D(G(z)): 0.0065 / 0.0221\n",
      "[0/1][469/782] Loss_D: 0.2038 Loss_G: 4.2524 D(x): 0.9703 D(G(z)): 0.1436 / 0.0258\n",
      "[0/1][470/782] Loss_D: 0.4022 Loss_G: 6.8785 D(x): 0.9623 D(G(z)): 0.2679 / 0.0020\n",
      "[0/1][471/782] Loss_D: 0.1173 Loss_G: 6.6282 D(x): 0.9495 D(G(z)): 0.0577 / 0.0024\n",
      "[0/1][472/782] Loss_D: 0.4461 Loss_G: 3.1074 D(x): 0.7415 D(G(z)): 0.0580 / 0.0629\n",
      "[0/1][473/782] Loss_D: 1.7129 Loss_G: 11.7605 D(x): 0.9012 D(G(z)): 0.7611 / 0.0000\n",
      "[0/1][474/782] Loss_D: 3.5249 Loss_G: 5.8284 D(x): 0.0844 D(G(z)): 0.0011 / 0.0306\n",
      "[0/1][475/782] Loss_D: 1.1679 Loss_G: 2.5436 D(x): 0.7425 D(G(z)): 0.3085 / 0.1948\n",
      "[0/1][476/782] Loss_D: 2.8004 Loss_G: 5.8451 D(x): 0.8814 D(G(z)): 0.7939 / 0.0227\n",
      "[0/1][477/782] Loss_D: 1.3900 Loss_G: 5.1542 D(x): 0.5579 D(G(z)): 0.1750 / 0.0142\n",
      "[0/1][478/782] Loss_D: 1.4409 Loss_G: 2.7879 D(x): 0.4634 D(G(z)): 0.1901 / 0.1189\n",
      "[0/1][479/782] Loss_D: 0.6378 Loss_G: 4.0711 D(x): 0.8633 D(G(z)): 0.3334 / 0.0341\n",
      "[0/1][480/782] Loss_D: 0.4483 Loss_G: 4.4872 D(x): 0.8463 D(G(z)): 0.1990 / 0.0201\n",
      "[0/1][481/782] Loss_D: 0.4721 Loss_G: 3.9769 D(x): 0.8375 D(G(z)): 0.2070 / 0.0308\n",
      "[0/1][482/782] Loss_D: 0.6555 Loss_G: 3.4093 D(x): 0.7518 D(G(z)): 0.2213 / 0.0455\n",
      "[0/1][483/782] Loss_D: 0.7875 Loss_G: 4.5670 D(x): 0.8411 D(G(z)): 0.3957 / 0.0221\n",
      "[0/1][484/782] Loss_D: 0.5086 Loss_G: 4.1910 D(x): 0.7644 D(G(z)): 0.1481 / 0.0252\n",
      "[0/1][485/782] Loss_D: 1.0640 Loss_G: 1.6415 D(x): 0.5369 D(G(z)): 0.1743 / 0.2526\n",
      "[0/1][486/782] Loss_D: 1.3289 Loss_G: 6.1761 D(x): 0.8957 D(G(z)): 0.6458 / 0.0046\n",
      "[0/1][487/782] Loss_D: 1.0838 Loss_G: 4.3387 D(x): 0.5115 D(G(z)): 0.0334 / 0.0243\n",
      "[0/1][488/782] Loss_D: 0.4478 Loss_G: 2.4537 D(x): 0.7609 D(G(z)): 0.0929 / 0.1189\n",
      "[0/1][489/782] Loss_D: 0.7426 Loss_G: 4.7198 D(x): 0.8733 D(G(z)): 0.3666 / 0.0200\n",
      "[0/1][490/782] Loss_D: 0.2649 Loss_G: 5.1268 D(x): 0.8795 D(G(z)): 0.0876 / 0.0098\n",
      "[0/1][491/782] Loss_D: 0.1881 Loss_G: 4.6454 D(x): 0.9003 D(G(z)): 0.0582 / 0.0170\n",
      "[0/1][492/782] Loss_D: 0.3025 Loss_G: 3.5443 D(x): 0.8368 D(G(z)): 0.0713 / 0.0434\n",
      "[0/1][493/782] Loss_D: 0.4899 Loss_G: 4.2761 D(x): 0.8652 D(G(z)): 0.2484 / 0.0236\n",
      "[0/1][494/782] Loss_D: 0.5466 Loss_G: 3.9960 D(x): 0.7660 D(G(z)): 0.1759 / 0.0293\n",
      "[0/1][495/782] Loss_D: 0.4042 Loss_G: 3.5057 D(x): 0.7896 D(G(z)): 0.1037 / 0.0393\n",
      "[0/1][496/782] Loss_D: 0.5337 Loss_G: 4.9961 D(x): 0.8831 D(G(z)): 0.3020 / 0.0102\n",
      "[0/1][497/782] Loss_D: 0.8103 Loss_G: 2.8036 D(x): 0.5822 D(G(z)): 0.0807 / 0.1028\n",
      "[0/1][498/782] Loss_D: 0.7762 Loss_G: 4.8722 D(x): 0.8375 D(G(z)): 0.3640 / 0.0167\n",
      "[0/1][499/782] Loss_D: 0.4664 Loss_G: 4.3954 D(x): 0.7452 D(G(z)): 0.0868 / 0.0220\n",
      "[0/1][500/782] Loss_D: 0.3827 Loss_G: 3.3250 D(x): 0.8154 D(G(z)): 0.1166 / 0.0647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][501/782] Loss_D: 0.3664 Loss_G: 4.3006 D(x): 0.9117 D(G(z)): 0.2127 / 0.0202\n",
      "[0/1][502/782] Loss_D: 0.4245 Loss_G: 4.4822 D(x): 0.8446 D(G(z)): 0.1862 / 0.0217\n",
      "[0/1][503/782] Loss_D: 0.2371 Loss_G: 4.5760 D(x): 0.8792 D(G(z)): 0.0831 / 0.0202\n",
      "[0/1][504/782] Loss_D: 0.6607 Loss_G: 2.1833 D(x): 0.6531 D(G(z)): 0.1107 / 0.1744\n",
      "[0/1][505/782] Loss_D: 1.0769 Loss_G: 6.9779 D(x): 0.9319 D(G(z)): 0.5527 / 0.0014\n",
      "[0/1][506/782] Loss_D: 0.7180 Loss_G: 4.6744 D(x): 0.5603 D(G(z)): 0.0119 / 0.0148\n",
      "[0/1][507/782] Loss_D: 0.2603 Loss_G: 2.7236 D(x): 0.8684 D(G(z)): 0.0923 / 0.0930\n",
      "[0/1][508/782] Loss_D: 0.7547 Loss_G: 4.2939 D(x): 0.8409 D(G(z)): 0.3556 / 0.0239\n",
      "[0/1][509/782] Loss_D: 0.5030 Loss_G: 3.4005 D(x): 0.7761 D(G(z)): 0.1393 / 0.0435\n",
      "[0/1][510/782] Loss_D: 0.4978 Loss_G: 2.1672 D(x): 0.7313 D(G(z)): 0.1005 / 0.1454\n",
      "[0/1][511/782] Loss_D: 0.8231 Loss_G: 6.4561 D(x): 0.9356 D(G(z)): 0.4815 / 0.0026\n",
      "[0/1][512/782] Loss_D: 0.5855 Loss_G: 5.2210 D(x): 0.6393 D(G(z)): 0.0087 / 0.0106\n",
      "[0/1][513/782] Loss_D: 0.3489 Loss_G: 1.9978 D(x): 0.7846 D(G(z)): 0.0576 / 0.1868\n",
      "[0/1][514/782] Loss_D: 0.4525 Loss_G: 4.7418 D(x): 0.9865 D(G(z)): 0.3126 / 0.0153\n",
      "[0/1][515/782] Loss_D: 0.3219 Loss_G: 4.9869 D(x): 0.9174 D(G(z)): 0.1377 / 0.0110\n",
      "[0/1][516/782] Loss_D: 0.1361 Loss_G: 5.0420 D(x): 0.9380 D(G(z)): 0.0599 / 0.0151\n",
      "[0/1][517/782] Loss_D: 0.7665 Loss_G: 1.5899 D(x): 0.6312 D(G(z)): 0.0802 / 0.2680\n",
      "[0/1][518/782] Loss_D: 1.0689 Loss_G: 6.1349 D(x): 0.9502 D(G(z)): 0.5344 / 0.0071\n",
      "[0/1][519/782] Loss_D: 0.9797 Loss_G: 3.2620 D(x): 0.5289 D(G(z)): 0.0421 / 0.0712\n",
      "[0/1][520/782] Loss_D: 0.6724 Loss_G: 2.2965 D(x): 0.7966 D(G(z)): 0.2540 / 0.1426\n",
      "[0/1][521/782] Loss_D: 0.8670 Loss_G: 5.6458 D(x): 0.9040 D(G(z)): 0.4531 / 0.0070\n",
      "[0/1][522/782] Loss_D: 1.0209 Loss_G: 2.5816 D(x): 0.4577 D(G(z)): 0.0319 / 0.1320\n",
      "[0/1][523/782] Loss_D: 0.5660 Loss_G: 4.1586 D(x): 0.9511 D(G(z)): 0.3545 / 0.0249\n",
      "[0/1][524/782] Loss_D: 0.2387 Loss_G: 4.7429 D(x): 0.9039 D(G(z)): 0.1076 / 0.0169\n",
      "[0/1][525/782] Loss_D: 0.3437 Loss_G: 3.8485 D(x): 0.8159 D(G(z)): 0.0960 / 0.0429\n",
      "[0/1][526/782] Loss_D: 0.4501 Loss_G: 3.3621 D(x): 0.8439 D(G(z)): 0.1869 / 0.0549\n",
      "[0/1][527/782] Loss_D: 0.3327 Loss_G: 4.5280 D(x): 0.9224 D(G(z)): 0.1999 / 0.0169\n",
      "[0/1][528/782] Loss_D: 0.2967 Loss_G: 3.9217 D(x): 0.8222 D(G(z)): 0.0637 / 0.0338\n",
      "[0/1][529/782] Loss_D: 0.3037 Loss_G: 4.2835 D(x): 0.9275 D(G(z)): 0.1823 / 0.0213\n",
      "[0/1][530/782] Loss_D: 0.6059 Loss_G: 3.1180 D(x): 0.7412 D(G(z)): 0.1680 / 0.0610\n",
      "[0/1][531/782] Loss_D: 0.7265 Loss_G: 6.0933 D(x): 0.8733 D(G(z)): 0.3720 / 0.0039\n",
      "[0/1][532/782] Loss_D: 1.3704 Loss_G: 0.9429 D(x): 0.3856 D(G(z)): 0.0260 / 0.4843\n",
      "[0/1][533/782] Loss_D: 1.9579 Loss_G: 8.5608 D(x): 0.9694 D(G(z)): 0.7292 / 0.0007\n",
      "[0/1][534/782] Loss_D: 1.3272 Loss_G: 5.3545 D(x): 0.4178 D(G(z)): 0.0071 / 0.0118\n",
      "[0/1][535/782] Loss_D: 0.5002 Loss_G: 2.5845 D(x): 0.7822 D(G(z)): 0.1565 / 0.1379\n",
      "[0/1][536/782] Loss_D: 1.0162 Loss_G: 5.2459 D(x): 0.8970 D(G(z)): 0.4956 / 0.0088\n",
      "[0/1][537/782] Loss_D: 0.8389 Loss_G: 2.9528 D(x): 0.5623 D(G(z)): 0.0820 / 0.0862\n",
      "[0/1][538/782] Loss_D: 0.5448 Loss_G: 3.0509 D(x): 0.8346 D(G(z)): 0.2524 / 0.0710\n",
      "[0/1][539/782] Loss_D: 0.4598 Loss_G: 4.9529 D(x): 0.9014 D(G(z)): 0.2632 / 0.0099\n",
      "[0/1][540/782] Loss_D: 0.3892 Loss_G: 3.6553 D(x): 0.7660 D(G(z)): 0.0636 / 0.0444\n",
      "[0/1][541/782] Loss_D: 0.3970 Loss_G: 3.7526 D(x): 0.8749 D(G(z)): 0.2002 / 0.0406\n",
      "[0/1][542/782] Loss_D: 0.5442 Loss_G: 4.0932 D(x): 0.8324 D(G(z)): 0.2614 / 0.0261\n",
      "[0/1][543/782] Loss_D: 0.7674 Loss_G: 2.6234 D(x): 0.6416 D(G(z)): 0.1758 / 0.1175\n",
      "[0/1][544/782] Loss_D: 0.7315 Loss_G: 4.8698 D(x): 0.8604 D(G(z)): 0.3702 / 0.0111\n",
      "[0/1][545/782] Loss_D: 0.5944 Loss_G: 2.8760 D(x): 0.6632 D(G(z)): 0.0680 / 0.0785\n",
      "[0/1][546/782] Loss_D: 0.6090 Loss_G: 4.4433 D(x): 0.8818 D(G(z)): 0.3383 / 0.0167\n",
      "[0/1][547/782] Loss_D: 0.7122 Loss_G: 2.3448 D(x): 0.6052 D(G(z)): 0.0903 / 0.1411\n",
      "[0/1][548/782] Loss_D: 0.6854 Loss_G: 5.9613 D(x): 0.9408 D(G(z)): 0.4229 / 0.0046\n",
      "[0/1][549/782] Loss_D: 0.6432 Loss_G: 3.2651 D(x): 0.6110 D(G(z)): 0.0334 / 0.0633\n",
      "[0/1][550/782] Loss_D: 0.3412 Loss_G: 3.2217 D(x): 0.9140 D(G(z)): 0.1948 / 0.0562\n",
      "[0/1][551/782] Loss_D: 0.5174 Loss_G: 4.8754 D(x): 0.8801 D(G(z)): 0.2826 / 0.0131\n",
      "[0/1][552/782] Loss_D: 0.7257 Loss_G: 2.4172 D(x): 0.6052 D(G(z)): 0.0740 / 0.1258\n",
      "[0/1][553/782] Loss_D: 1.0802 Loss_G: 7.3436 D(x): 0.9395 D(G(z)): 0.5713 / 0.0023\n",
      "[0/1][554/782] Loss_D: 1.4565 Loss_G: 3.0253 D(x): 0.3841 D(G(z)): 0.0212 / 0.1226\n",
      "[0/1][555/782] Loss_D: 0.5807 Loss_G: 4.9420 D(x): 0.8863 D(G(z)): 0.2895 / 0.0207\n",
      "[0/1][556/782] Loss_D: 0.9255 Loss_G: 3.1613 D(x): 0.6845 D(G(z)): 0.2123 / 0.1087\n",
      "[0/1][557/782] Loss_D: 0.5979 Loss_G: 4.7819 D(x): 0.8507 D(G(z)): 0.2414 / 0.0160\n",
      "[0/1][558/782] Loss_D: 0.4282 Loss_G: 2.8161 D(x): 0.7633 D(G(z)): 0.0920 / 0.1050\n",
      "[0/1][559/782] Loss_D: 0.6756 Loss_G: 3.3750 D(x): 0.8171 D(G(z)): 0.3041 / 0.0686\n",
      "[0/1][560/782] Loss_D: 0.6661 Loss_G: 4.2226 D(x): 0.7901 D(G(z)): 0.2837 / 0.0230\n",
      "[0/1][561/782] Loss_D: 0.8452 Loss_G: 1.1473 D(x): 0.5750 D(G(z)): 0.0924 / 0.3780\n",
      "[0/1][562/782] Loss_D: 1.0118 Loss_G: 5.1998 D(x): 0.9685 D(G(z)): 0.5670 / 0.0083\n",
      "[0/1][563/782] Loss_D: 1.1907 Loss_G: 0.7495 D(x): 0.4186 D(G(z)): 0.0546 / 0.5345\n",
      "[0/1][564/782] Loss_D: 1.4597 Loss_G: 6.7981 D(x): 0.9362 D(G(z)): 0.6837 / 0.0018\n",
      "[0/1][565/782] Loss_D: 2.0478 Loss_G: 3.0803 D(x): 0.2175 D(G(z)): 0.0052 / 0.1259\n",
      "[0/1][566/782] Loss_D: 0.6823 Loss_G: 3.0462 D(x): 0.9033 D(G(z)): 0.3176 / 0.0786\n",
      "[0/1][567/782] Loss_D: 0.5212 Loss_G: 4.5572 D(x): 0.8631 D(G(z)): 0.2290 / 0.0267\n",
      "[0/1][568/782] Loss_D: 0.3922 Loss_G: 3.6089 D(x): 0.7746 D(G(z)): 0.0846 / 0.0537\n",
      "[0/1][569/782] Loss_D: 0.4420 Loss_G: 3.5778 D(x): 0.8349 D(G(z)): 0.1918 / 0.0402\n",
      "[0/1][570/782] Loss_D: 0.6816 Loss_G: 2.6584 D(x): 0.6982 D(G(z)): 0.1666 / 0.0927\n",
      "[0/1][571/782] Loss_D: 0.3396 Loss_G: 4.2128 D(x): 0.9472 D(G(z)): 0.2299 / 0.0211\n",
      "[0/1][572/782] Loss_D: 0.3854 Loss_G: 3.7810 D(x): 0.8324 D(G(z)): 0.1427 / 0.0350\n",
      "[0/1][573/782] Loss_D: 0.3622 Loss_G: 3.6620 D(x): 0.8419 D(G(z)): 0.1484 / 0.0344\n",
      "[0/1][574/782] Loss_D: 0.3327 Loss_G: 3.1632 D(x): 0.8362 D(G(z)): 0.1205 / 0.0568\n",
      "[0/1][575/782] Loss_D: 0.3118 Loss_G: 4.0890 D(x): 0.9184 D(G(z)): 0.1847 / 0.0241\n",
      "[0/1][576/782] Loss_D: 0.5671 Loss_G: 2.7264 D(x): 0.7037 D(G(z)): 0.1170 / 0.0954\n",
      "[0/1][577/782] Loss_D: 0.6048 Loss_G: 5.4512 D(x): 0.9222 D(G(z)): 0.3724 / 0.0063\n",
      "[0/1][578/782] Loss_D: 0.4575 Loss_G: 4.1512 D(x): 0.7063 D(G(z)): 0.0304 / 0.0295\n",
      "[0/1][579/782] Loss_D: 0.2961 Loss_G: 2.4623 D(x): 0.8378 D(G(z)): 0.0754 / 0.1144\n",
      "[0/1][580/782] Loss_D: 0.7307 Loss_G: 4.6480 D(x): 0.9472 D(G(z)): 0.4386 / 0.0264\n",
      "[0/1][581/782] Loss_D: 0.9463 Loss_G: 1.3488 D(x): 0.5575 D(G(z)): 0.0839 / 0.3315\n",
      "[0/1][582/782] Loss_D: 1.3831 Loss_G: 8.3273 D(x): 0.9340 D(G(z)): 0.6431 / 0.0004\n",
      "[0/1][583/782] Loss_D: 1.4607 Loss_G: 5.5457 D(x): 0.3212 D(G(z)): 0.0023 / 0.0099\n",
      "[0/1][584/782] Loss_D: 0.3502 Loss_G: 1.9309 D(x): 0.7863 D(G(z)): 0.0380 / 0.2265\n",
      "[0/1][585/782] Loss_D: 0.7219 Loss_G: 4.5420 D(x): 0.9810 D(G(z)): 0.4335 / 0.0231\n",
      "[0/1][586/782] Loss_D: 0.2714 Loss_G: 5.1855 D(x): 0.9297 D(G(z)): 0.1520 / 0.0100\n",
      "[0/1][587/782] Loss_D: 0.3928 Loss_G: 3.8714 D(x): 0.7792 D(G(z)): 0.0587 / 0.0379\n",
      "[0/1][588/782] Loss_D: 0.6503 Loss_G: 3.5246 D(x): 0.8014 D(G(z)): 0.2690 / 0.0452\n",
      "[0/1][589/782] Loss_D: 0.4627 Loss_G: 3.2560 D(x): 0.7933 D(G(z)): 0.1678 / 0.0538\n",
      "[0/1][590/782] Loss_D: 0.5455 Loss_G: 4.9489 D(x): 0.8786 D(G(z)): 0.3066 / 0.0089\n",
      "[0/1][591/782] Loss_D: 0.5788 Loss_G: 2.8929 D(x): 0.6595 D(G(z)): 0.0802 / 0.0747\n",
      "[0/1][592/782] Loss_D: 0.6008 Loss_G: 5.6137 D(x): 0.9265 D(G(z)): 0.3750 / 0.0052\n",
      "[0/1][593/782] Loss_D: 0.6208 Loss_G: 3.6955 D(x): 0.6485 D(G(z)): 0.0521 / 0.0394\n",
      "[0/1][594/782] Loss_D: 0.4713 Loss_G: 4.6675 D(x): 0.8674 D(G(z)): 0.2418 / 0.0147\n",
      "[0/1][595/782] Loss_D: 0.4260 Loss_G: 3.7287 D(x): 0.7668 D(G(z)): 0.0928 / 0.0346\n",
      "[0/1][596/782] Loss_D: 0.3058 Loss_G: 4.0875 D(x): 0.8845 D(G(z)): 0.1507 / 0.0209\n",
      "[0/1][597/782] Loss_D: 0.2543 Loss_G: 4.4107 D(x): 0.8913 D(G(z)): 0.1136 / 0.0167\n",
      "[0/1][598/782] Loss_D: 0.2794 Loss_G: 3.9985 D(x): 0.8713 D(G(z)): 0.1071 / 0.0251\n",
      "[0/1][599/782] Loss_D: 0.3769 Loss_G: 5.6326 D(x): 0.9068 D(G(z)): 0.2165 / 0.0054\n",
      "[0/1][600/782] Loss_D: 0.2712 Loss_G: 4.4529 D(x): 0.8154 D(G(z)): 0.0426 / 0.0154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][601/782] Loss_D: 0.2834 Loss_G: 3.3693 D(x): 0.8809 D(G(z)): 0.1292 / 0.0426\n",
      "[0/1][602/782] Loss_D: 0.8014 Loss_G: 6.1535 D(x): 0.9327 D(G(z)): 0.4789 / 0.0052\n",
      "[0/1][603/782] Loss_D: 1.4815 Loss_G: 0.8986 D(x): 0.3384 D(G(z)): 0.0277 / 0.5039\n",
      "[0/1][604/782] Loss_D: 2.1275 Loss_G: 7.5631 D(x): 0.9477 D(G(z)): 0.7781 / 0.0012\n",
      "[0/1][605/782] Loss_D: 1.3904 Loss_G: 4.2363 D(x): 0.3463 D(G(z)): 0.0079 / 0.0276\n",
      "[0/1][606/782] Loss_D: 0.7975 Loss_G: 1.1045 D(x): 0.6112 D(G(z)): 0.1302 / 0.3823\n",
      "[0/1][607/782] Loss_D: 1.7659 Loss_G: 6.0084 D(x): 0.9585 D(G(z)): 0.7494 / 0.0064\n",
      "[0/1][608/782] Loss_D: 1.0994 Loss_G: 4.2747 D(x): 0.4748 D(G(z)): 0.0465 / 0.0205\n",
      "[0/1][609/782] Loss_D: 0.4971 Loss_G: 2.5694 D(x): 0.7343 D(G(z)): 0.0826 / 0.1207\n",
      "[0/1][610/782] Loss_D: 0.7496 Loss_G: 5.2125 D(x): 0.9251 D(G(z)): 0.4287 / 0.0094\n",
      "[0/1][611/782] Loss_D: 0.4490 Loss_G: 3.7542 D(x): 0.7335 D(G(z)): 0.0477 / 0.0364\n",
      "[0/1][612/782] Loss_D: 0.4362 Loss_G: 2.8736 D(x): 0.8121 D(G(z)): 0.1635 / 0.0722\n",
      "[0/1][613/782] Loss_D: 0.5799 Loss_G: 3.8811 D(x): 0.8626 D(G(z)): 0.3168 / 0.0274\n",
      "[0/1][614/782] Loss_D: 0.5156 Loss_G: 3.3854 D(x): 0.7439 D(G(z)): 0.1309 / 0.0510\n",
      "[0/1][615/782] Loss_D: 0.6177 Loss_G: 4.1301 D(x): 0.8635 D(G(z)): 0.3445 / 0.0238\n",
      "[0/1][616/782] Loss_D: 0.5412 Loss_G: 2.6704 D(x): 0.7016 D(G(z)): 0.0958 / 0.0912\n",
      "[0/1][617/782] Loss_D: 0.5094 Loss_G: 4.2060 D(x): 0.8934 D(G(z)): 0.2974 / 0.0297\n",
      "[0/1][618/782] Loss_D: 0.6658 Loss_G: 1.9590 D(x): 0.6315 D(G(z)): 0.0770 / 0.1816\n",
      "[0/1][619/782] Loss_D: 0.8155 Loss_G: 4.8360 D(x): 0.8278 D(G(z)): 0.4077 / 0.0174\n",
      "[0/1][620/782] Loss_D: 0.6033 Loss_G: 2.8984 D(x): 0.6769 D(G(z)): 0.0840 / 0.0760\n",
      "[0/1][621/782] Loss_D: 0.3372 Loss_G: 3.5467 D(x): 0.9107 D(G(z)): 0.1865 / 0.0447\n",
      "[0/1][622/782] Loss_D: 0.4111 Loss_G: 5.6619 D(x): 0.9331 D(G(z)): 0.2280 / 0.0055\n",
      "[0/1][623/782] Loss_D: 0.9046 Loss_G: 2.0702 D(x): 0.5042 D(G(z)): 0.0378 / 0.1736\n",
      "[0/1][624/782] Loss_D: 0.6618 Loss_G: 4.5923 D(x): 0.9384 D(G(z)): 0.3764 / 0.0172\n",
      "[0/1][625/782] Loss_D: 0.2670 Loss_G: 4.8714 D(x): 0.8963 D(G(z)): 0.1309 / 0.0117\n",
      "[0/1][626/782] Loss_D: 0.4658 Loss_G: 3.1040 D(x): 0.7403 D(G(z)): 0.0956 / 0.0687\n",
      "[0/1][627/782] Loss_D: 0.5329 Loss_G: 4.8171 D(x): 0.9214 D(G(z)): 0.3169 / 0.0129\n",
      "[0/1][628/782] Loss_D: 0.5973 Loss_G: 3.0733 D(x): 0.6738 D(G(z)): 0.0964 / 0.0689\n",
      "[0/1][629/782] Loss_D: 0.6253 Loss_G: 4.5660 D(x): 0.9119 D(G(z)): 0.3365 / 0.0160\n",
      "[0/1][630/782] Loss_D: 0.4375 Loss_G: 3.8728 D(x): 0.7901 D(G(z)): 0.1408 / 0.0317\n",
      "[0/1][631/782] Loss_D: 0.7893 Loss_G: 3.0046 D(x): 0.7202 D(G(z)): 0.2798 / 0.0670\n",
      "[0/1][632/782] Loss_D: 0.4898 Loss_G: 4.3262 D(x): 0.8431 D(G(z)): 0.2320 / 0.0213\n",
      "[0/1][633/782] Loss_D: 0.5267 Loss_G: 4.9839 D(x): 0.8594 D(G(z)): 0.2583 / 0.0109\n",
      "[0/1][634/782] Loss_D: 0.6272 Loss_G: 2.3217 D(x): 0.6339 D(G(z)): 0.0763 / 0.1214\n",
      "[0/1][635/782] Loss_D: 1.0840 Loss_G: 8.3442 D(x): 0.9701 D(G(z)): 0.5823 / 0.0006\n",
      "[0/1][636/782] Loss_D: 1.8049 Loss_G: 3.6997 D(x): 0.2733 D(G(z)): 0.0102 / 0.0561\n",
      "[0/1][637/782] Loss_D: 0.5515 Loss_G: 1.9748 D(x): 0.7827 D(G(z)): 0.2177 / 0.1792\n",
      "[0/1][638/782] Loss_D: 1.2379 Loss_G: 6.3220 D(x): 0.8619 D(G(z)): 0.5916 / 0.0031\n",
      "[0/1][639/782] Loss_D: 1.8111 Loss_G: 0.7435 D(x): 0.2586 D(G(z)): 0.0255 / 0.5546\n",
      "[0/1][640/782] Loss_D: 2.2626 Loss_G: 6.4985 D(x): 0.9975 D(G(z)): 0.8178 / 0.0037\n",
      "[0/1][641/782] Loss_D: 0.7573 Loss_G: 5.0891 D(x): 0.5842 D(G(z)): 0.0525 / 0.0156\n",
      "[0/1][642/782] Loss_D: 0.4325 Loss_G: 2.8427 D(x): 0.7615 D(G(z)): 0.0932 / 0.1046\n",
      "[0/1][643/782] Loss_D: 0.8455 Loss_G: 4.3368 D(x): 0.8772 D(G(z)): 0.4144 / 0.0221\n",
      "[0/1][644/782] Loss_D: 0.3353 Loss_G: 4.6283 D(x): 0.8710 D(G(z)): 0.1462 / 0.0154\n",
      "[0/1][645/782] Loss_D: 0.1985 Loss_G: 4.4045 D(x): 0.9025 D(G(z)): 0.0790 / 0.0188\n",
      "[0/1][646/782] Loss_D: 0.4861 Loss_G: 3.3827 D(x): 0.7857 D(G(z)): 0.1666 / 0.0451\n",
      "[0/1][647/782] Loss_D: 0.4874 Loss_G: 4.1177 D(x): 0.8465 D(G(z)): 0.2370 / 0.0256\n",
      "[0/1][648/782] Loss_D: 0.3912 Loss_G: 3.1063 D(x): 0.7707 D(G(z)): 0.0863 / 0.0643\n",
      "[0/1][649/782] Loss_D: 0.7116 Loss_G: 6.3302 D(x): 0.9174 D(G(z)): 0.4079 / 0.0024\n",
      "[0/1][650/782] Loss_D: 0.8475 Loss_G: 0.4524 D(x): 0.5780 D(G(z)): 0.0190 / 0.6873\n",
      "[0/1][651/782] Loss_D: 2.8700 Loss_G: 10.7786 D(x): 0.9754 D(G(z)): 0.9111 / 0.0001\n",
      "[0/1][652/782] Loss_D: 3.1645 Loss_G: 3.8642 D(x): 0.0977 D(G(z)): 0.0005 / 0.0579\n",
      "[0/1][653/782] Loss_D: 0.9530 Loss_G: 1.2348 D(x): 0.6579 D(G(z)): 0.2484 / 0.3612\n",
      "[0/1][654/782] Loss_D: 1.3442 Loss_G: 5.4901 D(x): 0.8789 D(G(z)): 0.6040 / 0.0105\n",
      "[0/1][655/782] Loss_D: 1.4134 Loss_G: 2.6230 D(x): 0.4014 D(G(z)): 0.0433 / 0.1341\n",
      "[0/1][656/782] Loss_D: 1.0197 Loss_G: 2.2712 D(x): 0.7925 D(G(z)): 0.4496 / 0.1409\n",
      "[0/1][657/782] Loss_D: 0.7771 Loss_G: 3.6555 D(x): 0.7908 D(G(z)): 0.3516 / 0.0446\n",
      "[0/1][658/782] Loss_D: 1.2276 Loss_G: 1.0723 D(x): 0.4370 D(G(z)): 0.1334 / 0.3974\n",
      "[0/1][659/782] Loss_D: 1.5686 Loss_G: 4.5616 D(x): 0.9021 D(G(z)): 0.6937 / 0.0182\n",
      "[0/1][660/782] Loss_D: 0.8680 Loss_G: 3.0686 D(x): 0.5195 D(G(z)): 0.0667 / 0.0712\n",
      "[0/1][661/782] Loss_D: 0.6101 Loss_G: 1.8614 D(x): 0.7410 D(G(z)): 0.2007 / 0.1999\n",
      "[0/1][662/782] Loss_D: 0.6612 Loss_G: 3.9898 D(x): 0.9290 D(G(z)): 0.4047 / 0.0236\n",
      "[0/1][663/782] Loss_D: 0.3924 Loss_G: 3.6978 D(x): 0.7838 D(G(z)): 0.0783 / 0.0404\n",
      "[0/1][664/782] Loss_D: 0.4489 Loss_G: 2.6637 D(x): 0.8209 D(G(z)): 0.1795 / 0.0952\n",
      "[0/1][665/782] Loss_D: 0.5068 Loss_G: 2.9675 D(x): 0.8457 D(G(z)): 0.2468 / 0.0686\n",
      "[0/1][666/782] Loss_D: 1.0581 Loss_G: 2.8055 D(x): 0.6509 D(G(z)): 0.3565 / 0.0790\n",
      "[0/1][667/782] Loss_D: 0.8875 Loss_G: 3.6702 D(x): 0.7349 D(G(z)): 0.3723 / 0.0353\n",
      "[0/1][668/782] Loss_D: 1.1306 Loss_G: 1.4364 D(x): 0.4958 D(G(z)): 0.1757 / 0.2904\n",
      "[0/1][669/782] Loss_D: 1.1492 Loss_G: 5.5487 D(x): 0.9268 D(G(z)): 0.5836 / 0.0077\n",
      "[0/1][670/782] Loss_D: 1.8020 Loss_G: 1.8769 D(x): 0.2356 D(G(z)): 0.0214 / 0.2457\n",
      "[0/1][671/782] Loss_D: 0.8470 Loss_G: 3.4790 D(x): 0.9526 D(G(z)): 0.4626 / 0.0538\n",
      "[0/1][672/782] Loss_D: 0.7047 Loss_G: 4.3539 D(x): 0.7902 D(G(z)): 0.2873 / 0.0226\n",
      "[0/1][673/782] Loss_D: 0.9331 Loss_G: 2.0418 D(x): 0.5155 D(G(z)): 0.0690 / 0.1671\n",
      "[0/1][674/782] Loss_D: 0.9700 Loss_G: 4.1683 D(x): 0.8450 D(G(z)): 0.5102 / 0.0208\n",
      "[0/1][675/782] Loss_D: 0.8941 Loss_G: 2.4347 D(x): 0.5328 D(G(z)): 0.1009 / 0.1222\n",
      "[0/1][676/782] Loss_D: 0.6687 Loss_G: 3.5719 D(x): 0.8420 D(G(z)): 0.3448 / 0.0379\n",
      "[0/1][677/782] Loss_D: 0.7194 Loss_G: 2.5523 D(x): 0.6323 D(G(z)): 0.1234 / 0.1071\n",
      "[0/1][678/782] Loss_D: 0.7600 Loss_G: 4.6078 D(x): 0.8796 D(G(z)): 0.3908 / 0.0142\n",
      "[0/1][679/782] Loss_D: 0.4623 Loss_G: 3.5610 D(x): 0.6965 D(G(z)): 0.0517 / 0.0425\n",
      "[0/1][680/782] Loss_D: 0.5381 Loss_G: 1.8344 D(x): 0.7126 D(G(z)): 0.1154 / 0.2009\n",
      "[0/1][681/782] Loss_D: 0.6284 Loss_G: 3.9634 D(x): 0.9281 D(G(z)): 0.3829 / 0.0255\n",
      "[0/1][682/782] Loss_D: 0.4858 Loss_G: 3.5657 D(x): 0.7795 D(G(z)): 0.1562 / 0.0386\n",
      "[0/1][683/782] Loss_D: 0.4328 Loss_G: 2.9696 D(x): 0.7888 D(G(z)): 0.1454 / 0.0630\n",
      "[0/1][684/782] Loss_D: 0.3024 Loss_G: 3.9331 D(x): 0.9310 D(G(z)): 0.1944 / 0.0258\n",
      "[0/1][685/782] Loss_D: 0.5500 Loss_G: 2.4390 D(x): 0.6764 D(G(z)): 0.0990 / 0.1168\n",
      "[0/1][686/782] Loss_D: 0.5527 Loss_G: 4.0672 D(x): 0.8991 D(G(z)): 0.3219 / 0.0229\n",
      "[0/1][687/782] Loss_D: 0.4769 Loss_G: 3.0491 D(x): 0.7166 D(G(z)): 0.0856 / 0.0672\n",
      "[0/1][688/782] Loss_D: 0.5486 Loss_G: 3.5485 D(x): 0.8098 D(G(z)): 0.2437 / 0.0388\n",
      "[0/1][689/782] Loss_D: 0.5005 Loss_G: 3.5081 D(x): 0.7782 D(G(z)): 0.1765 / 0.0435\n",
      "[0/1][690/782] Loss_D: 0.5115 Loss_G: 3.1449 D(x): 0.7622 D(G(z)): 0.1669 / 0.0660\n",
      "[0/1][691/782] Loss_D: 0.6830 Loss_G: 6.1922 D(x): 0.9253 D(G(z)): 0.4018 / 0.0032\n",
      "[0/1][692/782] Loss_D: 0.8524 Loss_G: 4.0143 D(x): 0.5224 D(G(z)): 0.0207 / 0.0418\n",
      "[0/1][693/782] Loss_D: 0.3642 Loss_G: 3.1448 D(x): 0.8852 D(G(z)): 0.1757 / 0.0640\n",
      "[0/1][694/782] Loss_D: 0.8573 Loss_G: 5.0201 D(x): 0.8578 D(G(z)): 0.4136 / 0.0121\n",
      "[0/1][695/782] Loss_D: 0.5951 Loss_G: 3.7218 D(x): 0.6967 D(G(z)): 0.1127 / 0.0415\n",
      "[0/1][696/782] Loss_D: 0.8334 Loss_G: 3.0615 D(x): 0.7200 D(G(z)): 0.3019 / 0.0665\n",
      "[0/1][697/782] Loss_D: 0.9443 Loss_G: 4.0723 D(x): 0.7638 D(G(z)): 0.3993 / 0.0255\n",
      "[0/1][698/782] Loss_D: 0.7357 Loss_G: 2.7821 D(x): 0.6292 D(G(z)): 0.1232 / 0.0859\n",
      "[0/1][699/782] Loss_D: 0.7217 Loss_G: 4.3148 D(x): 0.8569 D(G(z)): 0.3544 / 0.0224\n",
      "[0/1][700/782] Loss_D: 0.5420 Loss_G: 4.2661 D(x): 0.7893 D(G(z)): 0.1708 / 0.0206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][701/782] Loss_D: 0.6537 Loss_G: 3.8301 D(x): 0.7403 D(G(z)): 0.2202 / 0.0369\n",
      "[0/1][702/782] Loss_D: 0.5063 Loss_G: 4.6598 D(x): 0.8439 D(G(z)): 0.2253 / 0.0136\n",
      "[0/1][703/782] Loss_D: 0.6971 Loss_G: 5.6531 D(x): 0.7863 D(G(z)): 0.2965 / 0.0058\n",
      "[0/1][704/782] Loss_D: 0.8235 Loss_G: 3.6311 D(x): 0.6138 D(G(z)): 0.1634 / 0.0413\n",
      "[0/1][705/782] Loss_D: 0.5540 Loss_G: 4.1529 D(x): 0.7982 D(G(z)): 0.2376 / 0.0252\n",
      "[0/1][706/782] Loss_D: 0.5003 Loss_G: 4.7362 D(x): 0.8298 D(G(z)): 0.2124 / 0.0150\n",
      "[0/1][707/782] Loss_D: 0.6762 Loss_G: 3.4996 D(x): 0.6959 D(G(z)): 0.1874 / 0.0456\n",
      "[0/1][708/782] Loss_D: 0.5618 Loss_G: 5.5434 D(x): 0.8811 D(G(z)): 0.3088 / 0.0059\n",
      "[0/1][709/782] Loss_D: 0.3414 Loss_G: 4.4233 D(x): 0.8070 D(G(z)): 0.0887 / 0.0186\n",
      "[0/1][710/782] Loss_D: 0.6704 Loss_G: 4.3761 D(x): 0.7957 D(G(z)): 0.2861 / 0.0196\n",
      "[0/1][711/782] Loss_D: 0.7317 Loss_G: 6.4533 D(x): 0.8375 D(G(z)): 0.3438 / 0.0034\n",
      "[0/1][712/782] Loss_D: 1.0046 Loss_G: 1.8049 D(x): 0.4923 D(G(z)): 0.0397 / 0.2123\n",
      "[0/1][713/782] Loss_D: 1.3214 Loss_G: 7.4412 D(x): 0.8845 D(G(z)): 0.6363 / 0.0012\n",
      "[0/1][714/782] Loss_D: 0.6193 Loss_G: 5.3013 D(x): 0.6213 D(G(z)): 0.0137 / 0.0095\n",
      "[0/1][715/782] Loss_D: 0.2315 Loss_G: 3.3789 D(x): 0.8792 D(G(z)): 0.0543 / 0.0504\n",
      "[0/1][716/782] Loss_D: 1.1675 Loss_G: 7.8455 D(x): 0.9394 D(G(z)): 0.5954 / 0.0007\n",
      "[0/1][717/782] Loss_D: 1.2375 Loss_G: 4.3401 D(x): 0.4363 D(G(z)): 0.0141 / 0.0273\n",
      "[0/1][718/782] Loss_D: 0.4148 Loss_G: 3.2510 D(x): 0.8559 D(G(z)): 0.1966 / 0.0623\n",
      "[0/1][719/782] Loss_D: 0.5624 Loss_G: 5.8987 D(x): 0.9116 D(G(z)): 0.3294 / 0.0039\n",
      "[0/1][720/782] Loss_D: 0.8657 Loss_G: 2.2674 D(x): 0.5251 D(G(z)): 0.0516 / 0.1483\n",
      "[0/1][721/782] Loss_D: 0.6088 Loss_G: 6.2970 D(x): 0.9717 D(G(z)): 0.3929 / 0.0030\n",
      "[0/1][722/782] Loss_D: 0.3007 Loss_G: 5.2329 D(x): 0.8017 D(G(z)): 0.0327 / 0.0093\n",
      "[0/1][723/782] Loss_D: 0.5514 Loss_G: 2.1264 D(x): 0.7010 D(G(z)): 0.0856 / 0.1776\n",
      "[0/1][724/782] Loss_D: 1.1868 Loss_G: 6.9050 D(x): 0.9754 D(G(z)): 0.5901 / 0.0026\n",
      "[0/1][725/782] Loss_D: 0.7999 Loss_G: 4.7175 D(x): 0.5728 D(G(z)): 0.0342 / 0.0242\n",
      "[0/1][726/782] Loss_D: 0.3562 Loss_G: 3.0731 D(x): 0.8501 D(G(z)): 0.1357 / 0.0758\n",
      "[0/1][727/782] Loss_D: 0.6991 Loss_G: 5.3459 D(x): 0.9447 D(G(z)): 0.4220 / 0.0081\n",
      "[0/1][728/782] Loss_D: 0.5495 Loss_G: 3.7806 D(x): 0.7006 D(G(z)): 0.0690 / 0.0368\n",
      "[0/1][729/782] Loss_D: 0.5090 Loss_G: 3.4784 D(x): 0.8312 D(G(z)): 0.2271 / 0.0416\n",
      "[0/1][730/782] Loss_D: 0.5812 Loss_G: 4.9182 D(x): 0.8581 D(G(z)): 0.2854 / 0.0125\n",
      "[0/1][731/782] Loss_D: 0.3123 Loss_G: 4.3275 D(x): 0.8060 D(G(z)): 0.0454 / 0.0236\n",
      "[0/1][732/782] Loss_D: 0.2411 Loss_G: 3.4130 D(x): 0.8629 D(G(z)): 0.0730 / 0.0595\n",
      "[0/1][733/782] Loss_D: 0.7707 Loss_G: 6.5207 D(x): 0.9120 D(G(z)): 0.4322 / 0.0023\n",
      "[0/1][734/782] Loss_D: 0.9117 Loss_G: 3.2106 D(x): 0.4901 D(G(z)): 0.0108 / 0.0751\n",
      "[0/1][735/782] Loss_D: 0.7347 Loss_G: 5.2573 D(x): 0.9116 D(G(z)): 0.4071 / 0.0124\n",
      "[0/1][736/782] Loss_D: 0.4719 Loss_G: 4.4191 D(x): 0.7462 D(G(z)): 0.0980 / 0.0270\n",
      "[0/1][737/782] Loss_D: 0.3568 Loss_G: 3.9898 D(x): 0.8595 D(G(z)): 0.1518 / 0.0362\n",
      "[0/1][738/782] Loss_D: 0.4327 Loss_G: 5.0453 D(x): 0.8881 D(G(z)): 0.2353 / 0.0120\n",
      "[0/1][739/782] Loss_D: 0.3375 Loss_G: 4.2812 D(x): 0.8419 D(G(z)): 0.1150 / 0.0256\n",
      "[0/1][740/782] Loss_D: 0.6471 Loss_G: 2.2721 D(x): 0.6825 D(G(z)): 0.1640 / 0.1627\n",
      "[0/1][741/782] Loss_D: 1.2388 Loss_G: 4.1381 D(x): 0.6921 D(G(z)): 0.4784 / 0.0288\n",
      "[0/1][742/782] Loss_D: 0.6341 Loss_G: 2.8847 D(x): 0.6714 D(G(z)): 0.1405 / 0.0995\n",
      "[0/1][743/782] Loss_D: 0.7993 Loss_G: 4.6745 D(x): 0.8576 D(G(z)): 0.3672 / 0.0178\n",
      "[0/1][744/782] Loss_D: 0.5572 Loss_G: 3.3260 D(x): 0.7286 D(G(z)): 0.1449 / 0.0645\n",
      "[0/1][745/782] Loss_D: 0.6952 Loss_G: 3.0547 D(x): 0.7401 D(G(z)): 0.2370 / 0.0845\n",
      "[0/1][746/782] Loss_D: 0.6914 Loss_G: 4.6455 D(x): 0.8333 D(G(z)): 0.3421 / 0.0164\n",
      "[0/1][747/782] Loss_D: 0.9094 Loss_G: 1.3292 D(x): 0.5207 D(G(z)): 0.0806 / 0.3395\n",
      "[0/1][748/782] Loss_D: 1.2808 Loss_G: 7.0644 D(x): 0.9407 D(G(z)): 0.5988 / 0.0018\n",
      "[0/1][749/782] Loss_D: 1.6228 Loss_G: 2.4146 D(x): 0.2791 D(G(z)): 0.0103 / 0.1778\n",
      "[0/1][750/782] Loss_D: 0.5091 Loss_G: 3.4793 D(x): 0.9255 D(G(z)): 0.3175 / 0.0458\n",
      "[0/1][751/782] Loss_D: 0.4222 Loss_G: 5.5511 D(x): 0.9183 D(G(z)): 0.2411 / 0.0062\n",
      "[0/1][752/782] Loss_D: 0.8045 Loss_G: 1.6307 D(x): 0.5474 D(G(z)): 0.0544 / 0.2707\n",
      "[0/1][753/782] Loss_D: 0.9053 Loss_G: 5.8571 D(x): 0.9440 D(G(z)): 0.4888 / 0.0054\n",
      "[0/1][754/782] Loss_D: 0.8439 Loss_G: 2.6319 D(x): 0.5172 D(G(z)): 0.0189 / 0.1497\n",
      "[0/1][755/782] Loss_D: 0.6999 Loss_G: 4.5560 D(x): 0.9445 D(G(z)): 0.3859 / 0.0253\n",
      "[0/1][756/782] Loss_D: 0.5529 Loss_G: 3.1189 D(x): 0.7015 D(G(z)): 0.1171 / 0.0810\n",
      "[0/1][757/782] Loss_D: 0.7512 Loss_G: 2.8775 D(x): 0.7318 D(G(z)): 0.2671 / 0.0877\n",
      "[0/1][758/782] Loss_D: 0.7604 Loss_G: 4.3309 D(x): 0.7966 D(G(z)): 0.3320 / 0.0228\n",
      "[0/1][759/782] Loss_D: 0.7295 Loss_G: 2.6298 D(x): 0.6704 D(G(z)): 0.1189 / 0.1020\n",
      "[0/1][760/782] Loss_D: 0.5349 Loss_G: 3.5193 D(x): 0.8605 D(G(z)): 0.2782 / 0.0389\n",
      "[0/1][761/782] Loss_D: 0.9732 Loss_G: 1.7212 D(x): 0.5469 D(G(z)): 0.1738 / 0.2358\n",
      "[0/1][762/782] Loss_D: 1.0042 Loss_G: 6.0374 D(x): 0.9483 D(G(z)): 0.5481 / 0.0044\n",
      "[0/1][763/782] Loss_D: 1.5244 Loss_G: 2.8215 D(x): 0.2879 D(G(z)): 0.0219 / 0.1139\n",
      "[0/1][764/782] Loss_D: 0.4875 Loss_G: 2.8407 D(x): 0.8997 D(G(z)): 0.2618 / 0.0963\n",
      "[0/1][765/782] Loss_D: 0.4081 Loss_G: 4.7367 D(x): 0.9041 D(G(z)): 0.2390 / 0.0109\n",
      "[0/1][766/782] Loss_D: 0.3577 Loss_G: 3.6357 D(x): 0.7785 D(G(z)): 0.0595 / 0.0379\n",
      "[0/1][767/782] Loss_D: 0.3375 Loss_G: 3.5902 D(x): 0.8714 D(G(z)): 0.1648 / 0.0359\n",
      "[0/1][768/782] Loss_D: 0.4711 Loss_G: 3.5455 D(x): 0.8122 D(G(z)): 0.1814 / 0.0440\n",
      "[0/1][769/782] Loss_D: 0.5480 Loss_G: 3.5740 D(x): 0.7951 D(G(z)): 0.2287 / 0.0392\n",
      "[0/1][770/782] Loss_D: 0.3870 Loss_G: 4.4286 D(x): 0.8673 D(G(z)): 0.1880 / 0.0169\n",
      "[0/1][771/782] Loss_D: 0.3754 Loss_G: 3.2297 D(x): 0.7558 D(G(z)): 0.0523 / 0.0638\n",
      "[0/1][772/782] Loss_D: 0.4730 Loss_G: 3.5318 D(x): 0.8667 D(G(z)): 0.2381 / 0.0459\n",
      "[0/1][773/782] Loss_D: 0.3543 Loss_G: 3.4623 D(x): 0.8145 D(G(z)): 0.1041 / 0.0452\n",
      "[0/1][774/782] Loss_D: 0.2891 Loss_G: 4.1292 D(x): 0.8953 D(G(z)): 0.1511 / 0.0209\n",
      "[0/1][775/782] Loss_D: 0.2370 Loss_G: 3.7160 D(x): 0.8532 D(G(z)): 0.0551 / 0.0394\n",
      "[0/1][776/782] Loss_D: 0.2313 Loss_G: 3.4543 D(x): 0.9035 D(G(z)): 0.1082 / 0.0465\n",
      "[0/1][777/782] Loss_D: 0.4198 Loss_G: 4.5526 D(x): 0.9308 D(G(z)): 0.2620 / 0.0141\n",
      "[0/1][778/782] Loss_D: 0.3008 Loss_G: 4.2415 D(x): 0.8230 D(G(z)): 0.0670 / 0.0212\n",
      "[0/1][779/782] Loss_D: 0.4213 Loss_G: 3.5531 D(x): 0.8333 D(G(z)): 0.1828 / 0.0433\n",
      "[0/1][780/782] Loss_D: 0.3210 Loss_G: 3.9013 D(x): 0.8921 D(G(z)): 0.1688 / 0.0273\n",
      "[0/1][781/782] Loss_D: 0.5100 Loss_G: 2.8599 D(x): 0.7244 D(G(z)): 0.1263 / 0.0782\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if opt.dry_run:\n",
    "    opt.niter = 1\n",
    "\n",
    "for epoch in range(opt.niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label,\n",
    "                           dtype=real_cpu.dtype, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            fretchet_dist=calculate_fretchet(real_cpu,fake,model)\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f, FID:%.4f'\n",
    "              % (epoch, opt.niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2,  fretchet_dist))\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % opt.outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                    normalize=True)\n",
    "\n",
    "        if opt.dry_run:\n",
    "            break\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        vutils.save_image(fake.detach(),\n",
    "                '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid.inception import InceptionV3\n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "model = InceptionV3([block_idx]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionV3(\n",
       "  (blocks): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicConv2d(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): FIDInceptionA(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_2): BasicConv2d(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): FIDInceptionA(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_2): BasicConv2d(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): FIDInceptionA(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_2): BasicConv2d(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InceptionB(\n",
       "        (branch3x3): BasicConv2d(\n",
       "          (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): FIDInceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): FIDInceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): FIDInceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): FIDInceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InceptionD(\n",
       "        (branch3x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_3): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_4): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): FIDInceptionE_1(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): FIDInceptionE_2(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n",
    "                    cuda=False):\n",
    "    model.eval()\n",
    "    act=np.empty((len(images), dims))\n",
    "    \n",
    "    if cuda:\n",
    "        batch=images.to(device)\n",
    "    else:\n",
    "        batch=images\n",
    "    pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
    "    \n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    \n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1) +\n",
    "            np.trace(sigma2) - 2 * tr_covmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fretchet(images_real,images_fake,model):\n",
    "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
    "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
    "    \n",
    "     \"\"\"get fretched distance\"\"\"\n",
    "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
    "     return fid_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434.27731065758974"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fretchet_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader, 0):\n",
    "    real_cpu = data[0].to(device)\n",
    "    vutils.save_image(real_cpu,\n",
    "        '%s/real_samples.png' % opt.outf,\n",
    "        normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [01:21,  9.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, data in tqdm.tqdm(enumerate(dataloader, 0)):\n",
    "    noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "    fake = netG(noise)\n",
    "    vutils.save_image(fake.detach(),\n",
    "            '%s/fake_samples_epoch_%03d_%d.png' % ('result_image/fake_samples', epoch, i),\n",
    "            normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid import fid_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*64*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.21875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "526 / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vutils??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53932800"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*3*530*530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786432"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*3*64*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.5791015625"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53932800 / 786432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b /opt/conda/envs/stylegan2/lib/python3.9/site-packages/pytorch_fid/fid_score.py:128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan2",
   "language": "python",
   "name": "stylegan2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
